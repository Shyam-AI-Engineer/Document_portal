{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c52b0b78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n"
     ]
    }
   ],
   "source": [
    "print(\"hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48571afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc5fe572",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "144e36f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3617d4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model=ChatGroq(model=\"qwen/qwen3-32b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75ab6341",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<think>\\nOkay, so the question is asking for the capital of France. Hmm, I think I remember that France's capital is Paris. Let me just make sure I'm not mixing it up with another country. I know that Paris is a major city and it's in France. I've heard of the Eiffel Tower and the Louvre being in Paris. Yeah, that's right. I don't think any other cities in France are capitals of other countries. For example, Germany's capital is Berlin, Spain's is Madrid, Italy's is Rome. So Paris must be the correct answer here. Wait, is there any chance I'm confusing it with another French-speaking country? Like Belgium's capital is Brussels, but that's not in France. Yeah, I think Paris is definitely the capital of France. I'll go with that.\\n</think>\\n\\nThe capital of France is **Paris**. \\n\\nParis is renowned worldwide for its cultural landmarks such as the Eiffel Tower, the Louvre Museum, and its rich history as a center of art, fashion, and gastronomy. It serves as the political, economic, and cultural hub of France. \\n\\n**Answer:** Paris.\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(\"What is the capital of France?\").content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0975484",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6eba59ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model=GoogleGenerativeAIEmbeddings(model=\"gemini-embedding-001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2af2598",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.03472332,\n",
       " 0.016623348,\n",
       " 0.017626096,\n",
       " -0.072375,\n",
       " -0.0281858,\n",
       " -0.017080193,\n",
       " -0.013316695,\n",
       " -0.011405033,\n",
       " 0.03533736,\n",
       " -0.020505501,\n",
       " -0.037189905,\n",
       " 0.003661672,\n",
       " -0.00963541,\n",
       " -0.014644037,\n",
       " 0.12218623,\n",
       " -0.008353268,\n",
       " -0.009492872,\n",
       " 0.017511755,\n",
       " -0.009100395,\n",
       " -0.00957398,\n",
       " -0.01709476,\n",
       " 0.003767815,\n",
       " 0.007509254,\n",
       " -0.0031589437,\n",
       " -0.0023174123,\n",
       " 0.021491988,\n",
       " -0.005579307,\n",
       " 0.004666332,\n",
       " 0.004514253,\n",
       " 0.042698465,\n",
       " 0.0083817,\n",
       " -0.011801647,\n",
       " -0.0034817075,\n",
       " -0.005522123,\n",
       " 0.02441637,\n",
       " -0.008408846,\n",
       " 0.0048623946,\n",
       " -0.018507626,\n",
       " -0.0062745763,\n",
       " 0.0016465912,\n",
       " -0.0013156065,\n",
       " -0.017395442,\n",
       " 0.025559647,\n",
       " -0.0025079353,\n",
       " 0.00069833535,\n",
       " -0.0074975346,\n",
       " 0.0047651953,\n",
       " -0.03696333,\n",
       " -0.026705028,\n",
       " 0.041696884,\n",
       " -0.008927896,\n",
       " 0.009216309,\n",
       " 0.0032405972,\n",
       " -0.17261928,\n",
       " -0.0005138008,\n",
       " 0.010442887,\n",
       " 0.0035214073,\n",
       " -0.009904886,\n",
       " 0.01913204,\n",
       " -0.027913455,\n",
       " -0.020697704,\n",
       " 0.0007436288,\n",
       " 0.002458415,\n",
       " -0.017847262,\n",
       " 0.01857969,\n",
       " 0.025336431,\n",
       " 0.00898924,\n",
       " 0.031155115,\n",
       " -0.005272584,\n",
       " -0.015486714,\n",
       " 0.021802641,\n",
       " 0.04206957,\n",
       " -0.022767039,\n",
       " -0.02659489,\n",
       " 0.012961395,\n",
       " 0.009301654,\n",
       " 0.0065554744,\n",
       " -0.0063041113,\n",
       " -0.015879568,\n",
       " 0.0040646577,\n",
       " -0.008814204,\n",
       " 0.014632396,\n",
       " -0.02501066,\n",
       " -0.013326307,\n",
       " -0.021774719,\n",
       " -0.041939743,\n",
       " -0.0025060787,\n",
       " -0.04123929,\n",
       " 0.01098791,\n",
       " 0.020988679,\n",
       " 0.018701538,\n",
       " 0.005156591,\n",
       " 0.014921706,\n",
       " -0.0074795904,\n",
       " -0.0082081435,\n",
       " 0.010202212,\n",
       " 0.013697578,\n",
       " -0.013411671,\n",
       " -0.000966949,\n",
       " -0.0007773471,\n",
       " 0.0045386967,\n",
       " -0.0022637544,\n",
       " -0.012320302,\n",
       " -0.0071902038,\n",
       " -0.004382978,\n",
       " -0.005622656,\n",
       " 0.0004995804,\n",
       " 0.0013958935,\n",
       " 0.009767147,\n",
       " 0.0027554932,\n",
       " -0.0087272655,\n",
       " -0.0024184254,\n",
       " -0.0025608847,\n",
       " 0.024073746,\n",
       " 0.017298277,\n",
       " -0.1489794,\n",
       " -0.005129656,\n",
       " 0.0109237,\n",
       " -0.02392762,\n",
       " 0.00067630823,\n",
       " 0.024600081,\n",
       " 0.012094909,\n",
       " 0.012745065,\n",
       " -0.010170578,\n",
       " 0.010357878,\n",
       " -0.0021158052,\n",
       " 0.012616093,\n",
       " -0.024401432,\n",
       " 0.023700707,\n",
       " 0.021163296,\n",
       " -0.0057402854,\n",
       " -0.004637997,\n",
       " -0.027112734,\n",
       " -0.012667589,\n",
       " 0.02782097,\n",
       " -0.0261016,\n",
       " 0.012542538,\n",
       " -0.00042292237,\n",
       " -0.00807791,\n",
       " -0.0110484585,\n",
       " 0.016478684,\n",
       " 0.020969456,\n",
       " -0.0059207915,\n",
       " -0.0060874326,\n",
       " -0.0020646304,\n",
       " -0.02814146,\n",
       " -0.04782883,\n",
       " -0.004984807,\n",
       " 0.026753576,\n",
       " 0.0014641619,\n",
       " 0.03522282,\n",
       " 0.013857278,\n",
       " -0.010219935,\n",
       " -0.014558912,\n",
       " -0.0154110575,\n",
       " -0.018846987,\n",
       " 0.025341872,\n",
       " -0.04020662,\n",
       " 0.04299616,\n",
       " -0.0119574815,\n",
       " -0.011644104,\n",
       " 0.002993579,\n",
       " -0.009465723,\n",
       " 0.0034996946,\n",
       " -0.023873432,\n",
       " 0.030298341,\n",
       " -0.0065292898,\n",
       " -0.0094441185,\n",
       " 0.026096292,\n",
       " 0.038880166,\n",
       " -0.016036822,\n",
       " -0.004466241,\n",
       " -0.013758593,\n",
       " 0.012764139,\n",
       " 0.012025729,\n",
       " -0.0006697049,\n",
       " 0.008098759,\n",
       " 0.013090301,\n",
       " 0.031233171,\n",
       " 0.0032054083,\n",
       " -0.006172015,\n",
       " -0.014015283,\n",
       " -0.0038957577,\n",
       " 0.025449002,\n",
       " 0.006444807,\n",
       " -0.014332894,\n",
       " 0.004869411,\n",
       " -0.001652028,\n",
       " -0.017295565,\n",
       " 0.025542663,\n",
       " -0.009453448,\n",
       " -0.008137921,\n",
       " -0.006887235,\n",
       " 0.002614993,\n",
       " 0.010626195,\n",
       " 0.0032908896,\n",
       " -0.016543379,\n",
       " 0.01707648,\n",
       " -0.020983983,\n",
       " 0.0005376961,\n",
       " -0.006081707,\n",
       " -0.014646286,\n",
       " 0.011645435,\n",
       " -0.0263086,\n",
       " 0.019433167,\n",
       " -0.018798057,\n",
       " 0.006382294,\n",
       " -0.027285755,\n",
       " -0.012159701,\n",
       " 0.03032815,\n",
       " 0.0123790335,\n",
       " -0.013520219,\n",
       " 0.028508699,\n",
       " -0.0038950937,\n",
       " 0.00679035,\n",
       " -0.030115295,\n",
       " -0.007637292,\n",
       " -0.013477366,\n",
       " -0.0036526828,\n",
       " 0.003367131,\n",
       " 0.018868653,\n",
       " -0.027157981,\n",
       " -0.036556505,\n",
       " 0.011968802,\n",
       " -0.0041641057,\n",
       " -0.0020479725,\n",
       " 0.0026599355,\n",
       " -0.018466972,\n",
       " -0.0033880442,\n",
       " 0.012641672,\n",
       " -0.018123358,\n",
       " -0.008335468,\n",
       " 0.026928127,\n",
       " -0.025873654,\n",
       " 0.008879314,\n",
       " 0.011465648,\n",
       " 0.016904434,\n",
       " -0.0027234016,\n",
       " -0.0054040933,\n",
       " -0.014772326,\n",
       " -0.005125906,\n",
       " -0.0013728897,\n",
       " 0.03574876,\n",
       " -1.496263e-05,\n",
       " 0.013368843,\n",
       " 0.01232948,\n",
       " 0.012612715,\n",
       " 0.0024312008,\n",
       " 0.020751374,\n",
       " -0.027049024,\n",
       " -0.027411032,\n",
       " 0.012040709,\n",
       " -0.0033530085,\n",
       " 0.026342135,\n",
       " -0.0051858267,\n",
       " 0.030345226,\n",
       " -0.017837087,\n",
       " -0.009227838,\n",
       " -0.0026374892,\n",
       " 0.0072117066,\n",
       " 0.00082680595,\n",
       " -0.01977238,\n",
       " 0.020522827,\n",
       " -0.008656299,\n",
       " -0.001973764,\n",
       " 0.009163512,\n",
       " -0.016685657,\n",
       " 0.01687141,\n",
       " -0.01672156,\n",
       " 0.02153017,\n",
       " 0.022188526,\n",
       " -0.027016224,\n",
       " -0.015777128,\n",
       " 0.011639852,\n",
       " -0.013295681,\n",
       " -0.017320968,\n",
       " -0.084105074,\n",
       " 0.0009980099,\n",
       " 0.025347099,\n",
       " -0.018027605,\n",
       " 0.0080349,\n",
       " 0.00024575123,\n",
       " -0.012845025,\n",
       " -0.01678927,\n",
       " -0.01446843,\n",
       " -0.009463349,\n",
       " 0.014699196,\n",
       " -0.02283457,\n",
       " 0.015471864,\n",
       " 0.018318234,\n",
       " -0.015194858,\n",
       " -0.019451737,\n",
       " 0.00394245,\n",
       " -0.011202905,\n",
       " 0.01131129,\n",
       " -0.007195287,\n",
       " -0.0048433784,\n",
       " -0.007578181,\n",
       " 0.012509196,\n",
       " 0.023088666,\n",
       " 0.036762513,\n",
       " -0.008325921,\n",
       " 0.022533191,\n",
       " 0.030817026,\n",
       " 0.023573084,\n",
       " 0.0035582918,\n",
       " -0.025709733,\n",
       " -0.013625739,\n",
       " -0.0026387346,\n",
       " -0.014043185,\n",
       " -0.0050453474,\n",
       " -0.009665815,\n",
       " 0.01607091,\n",
       " 0.009549982,\n",
       " -0.004826709,\n",
       " 0.011573502,\n",
       " -0.0073810327,\n",
       " 0.008029809,\n",
       " 0.00505066,\n",
       " -0.02037408,\n",
       " 0.0063107857,\n",
       " 0.0067934766,\n",
       " 0.008650588,\n",
       " 0.010081641,\n",
       " -0.032474052,\n",
       " -0.0027040006,\n",
       " 0.023464367,\n",
       " 0.015811525,\n",
       " 0.0038203879,\n",
       " -0.013930342,\n",
       " -0.0017105947,\n",
       " -0.006076753,\n",
       " 0.0018170746,\n",
       " 0.011044044,\n",
       " 0.0063644806,\n",
       " 0.007461747,\n",
       " 0.01910881,\n",
       " 0.008842695,\n",
       " -0.0104796495,\n",
       " 0.014233836,\n",
       " -0.013970029,\n",
       " -0.019971032,\n",
       " 0.0070648827,\n",
       " 0.009179152,\n",
       " -0.0013848556,\n",
       " 0.019293834,\n",
       " 0.011557694,\n",
       " -0.0019357895,\n",
       " -0.008868671,\n",
       " -0.010011886,\n",
       " 0.015952632,\n",
       " 0.028075265,\n",
       " 0.008555435,\n",
       " -0.022909798,\n",
       " 0.007839173,\n",
       " 0.010543233,\n",
       " 0.005148939,\n",
       " 0.018378483,\n",
       " 0.02375441,\n",
       " 0.0048642424,\n",
       " -0.0044257445,\n",
       " 0.021049097,\n",
       " 0.014373829,\n",
       " 0.014713888,\n",
       " -0.0016395862,\n",
       " -0.014401153,\n",
       " 0.011435733,\n",
       " -0.033450603,\n",
       " -0.016502868,\n",
       " 0.002419745,\n",
       " -0.013412796,\n",
       " 0.014781213,\n",
       " 0.010652607,\n",
       " 0.029901523,\n",
       " -0.017022444,\n",
       " 0.0052678166,\n",
       " 0.0031459648,\n",
       " -0.010736849,\n",
       " 0.034525648,\n",
       " 0.0126952985,\n",
       " -0.009862369,\n",
       " -0.01147084,\n",
       " 0.02978721,\n",
       " -0.002452499,\n",
       " -0.0025478879,\n",
       " -0.0006282518,\n",
       " 0.019359292,\n",
       " 0.0004976305,\n",
       " 0.02104304,\n",
       " -0.0023756085,\n",
       " -0.010077578,\n",
       " 0.03094241,\n",
       " 0.018225072,\n",
       " 0.0037346317,\n",
       " -0.0012397066,\n",
       " -0.013689001,\n",
       " 0.0020032215,\n",
       " -0.02607636,\n",
       " -0.037582412,\n",
       " -0.015160105,\n",
       " 0.0091663785,\n",
       " 0.0014720032,\n",
       " -0.029355139,\n",
       " -0.027457638,\n",
       " 0.013784792,\n",
       " 0.021489711,\n",
       " -0.009595106,\n",
       " 0.013935485,\n",
       " 0.021633068,\n",
       " -0.026020246,\n",
       " -0.00090742816,\n",
       " 0.017321864,\n",
       " -0.005385204,\n",
       " -0.0053981724,\n",
       " 0.0005237938,\n",
       " -0.008866138,\n",
       " -0.020897308,\n",
       " 0.0017008459,\n",
       " 0.005699534,\n",
       " 0.039668694,\n",
       " 0.017471073,\n",
       " -0.0008681236,\n",
       " 0.00042466633,\n",
       " 0.028104544,\n",
       " -0.0026053004,\n",
       " 0.0061616576,\n",
       " 0.022695249,\n",
       " -0.01758996,\n",
       " -0.026982494,\n",
       " 0.02092144,\n",
       " 0.0089702,\n",
       " -0.01324377,\n",
       " -0.02603907,\n",
       " 0.009454357,\n",
       " -0.011963382,\n",
       " 0.00089775375,\n",
       " 0.010746232,\n",
       " 0.019172188,\n",
       " -0.0040599494,\n",
       " -0.016085481,\n",
       " 0.024124464,\n",
       " 0.016356187,\n",
       " -0.04109421,\n",
       " 0.013687459,\n",
       " -0.013773554,\n",
       " -0.00798667,\n",
       " -0.00097397564,\n",
       " -0.0114466185,\n",
       " -0.0284737,\n",
       " -0.020489682,\n",
       " 0.00078399124,\n",
       " -0.0075115887,\n",
       " -0.016338374,\n",
       " -0.014864902,\n",
       " 0.0073115644,\n",
       " -0.011946165,\n",
       " 0.0050876336,\n",
       " -0.0049612764,\n",
       " -0.0073405434,\n",
       " 0.02689909,\n",
       " 0.01111431,\n",
       " 0.019024493,\n",
       " -0.014982571,\n",
       " 0.016423207,\n",
       " 0.000614489,\n",
       " 0.02719925,\n",
       " -0.01868937,\n",
       " 0.00012374246,\n",
       " 0.0054545957,\n",
       " -0.03431067,\n",
       " 0.0101801315,\n",
       " 0.0005123252,\n",
       " -0.018688634,\n",
       " 0.00091103633,\n",
       " 0.014778455,\n",
       " 0.0133777885,\n",
       " -0.02365304,\n",
       " -0.013237754,\n",
       " -0.0032934332,\n",
       " -0.04353243,\n",
       " -0.012184802,\n",
       " -0.014390456,\n",
       " -0.00072404556,\n",
       " -0.024257038,\n",
       " -0.0002933437,\n",
       " -0.0024709755,\n",
       " -0.0026414501,\n",
       " 0.015333084,\n",
       " -0.0041008242,\n",
       " -0.0015006326,\n",
       " 0.027216803,\n",
       " -0.019133354,\n",
       " 0.021096699,\n",
       " -0.00880537,\n",
       " -0.0051093586,\n",
       " -0.009494628,\n",
       " -0.00073447084,\n",
       " 0.017797312,\n",
       " 0.0033928424,\n",
       " -0.00066522975,\n",
       " 0.04246626,\n",
       " 0.010047135,\n",
       " -0.013578675,\n",
       " -0.001249083,\n",
       " 0.0024764482,\n",
       " 0.00048679323,\n",
       " 0.0013677592,\n",
       " 0.0075132195,\n",
       " 0.030938493,\n",
       " 0.0065220953,\n",
       " -0.0047181784,\n",
       " 0.0099684475,\n",
       " 0.027417967,\n",
       " 0.007843129,\n",
       " 0.007925577,\n",
       " 0.013959676,\n",
       " -0.005583488,\n",
       " -0.022255972,\n",
       " -0.004976209,\n",
       " 0.013802413,\n",
       " -0.039372332,\n",
       " -0.005945346,\n",
       " -0.006544413,\n",
       " 0.0016934556,\n",
       " 0.009553162,\n",
       " -0.007690198,\n",
       " -0.01939801,\n",
       " 0.029308373,\n",
       " -0.028693328,\n",
       " 0.014113017,\n",
       " 0.014647897,\n",
       " 0.033702966,\n",
       " 0.013373441,\n",
       " -0.03125519,\n",
       " -0.014551866,\n",
       " 0.00011649431,\n",
       " 0.0046795513,\n",
       " 0.025602957,\n",
       " 0.0052169827,\n",
       " 0.017640125,\n",
       " 0.005470827,\n",
       " -0.01023741,\n",
       " 0.005975175,\n",
       " -0.018126864,\n",
       " 0.008030207,\n",
       " -0.09119896,\n",
       " -0.0043378463,\n",
       " 0.019789077,\n",
       " -0.018328927,\n",
       " -0.0039921473,\n",
       " -0.0019481637,\n",
       " 0.016564025,\n",
       " -0.015175263,\n",
       " -0.010703932,\n",
       " -2.162054e-09,\n",
       " 0.015945587,\n",
       " -0.010396129,\n",
       " -0.020408818,\n",
       " 0.008891846,\n",
       " 0.0063368888,\n",
       " -0.017517846,\n",
       " -0.017237408,\n",
       " -0.0005829489,\n",
       " 0.009367293,\n",
       " -0.0019700036,\n",
       " 0.025358139,\n",
       " 0.025545036,\n",
       " 0.016268304,\n",
       " 0.021534655,\n",
       " -0.039552018,\n",
       " -0.0064646606,\n",
       " -0.015056669,\n",
       " -0.012800642,\n",
       " -0.02542254,\n",
       " -0.0013912148,\n",
       " -0.01645298,\n",
       " 0.01850912,\n",
       " -0.014215456,\n",
       " -0.012561697,\n",
       " 0.042688243,\n",
       " 0.0023163443,\n",
       " -0.012319963,\n",
       " -0.018474525,\n",
       " -0.006277615,\n",
       " 0.03216139,\n",
       " -0.005895159,\n",
       " 0.02569785,\n",
       " -0.0072686574,\n",
       " 0.020097237,\n",
       " 0.01726962,\n",
       " -0.013425107,\n",
       " -0.0061267703,\n",
       " -0.033881202,\n",
       " 0.009099212,\n",
       " 0.0039724703,\n",
       " -0.026287412,\n",
       " 0.015119061,\n",
       " 0.007827817,\n",
       " 0.018104622,\n",
       " 0.00094154576,\n",
       " -0.008852805,\n",
       " -0.00977888,\n",
       " -0.034297757,\n",
       " 0.010461071,\n",
       " -0.0075153434,\n",
       " -0.03868586,\n",
       " -0.015041302,\n",
       " -0.005697776,\n",
       " 0.0034182186,\n",
       " -0.015891725,\n",
       " -0.015979262,\n",
       " -0.011610553,\n",
       " -0.008396777,\n",
       " 0.0045173895,\n",
       " -0.029577097,\n",
       " -0.0003290079,\n",
       " 0.010329905,\n",
       " -0.04863303,\n",
       " -0.0024577763,\n",
       " -0.011255297,\n",
       " -0.013284915,\n",
       " 0.020574154,\n",
       " 0.004494812,\n",
       " 0.009979994,\n",
       " -0.009814616,\n",
       " -0.01934334,\n",
       " 0.03598013,\n",
       " -0.10007059,\n",
       " 0.009578866,\n",
       " -0.00024441644,\n",
       " -0.012630413,\n",
       " 0.005590081,\n",
       " 0.0024177702,\n",
       " -0.0029021592,\n",
       " -0.005216607,\n",
       " 0.0052818,\n",
       " 0.017126828,\n",
       " 0.0018035001,\n",
       " 0.0029852453,\n",
       " 0.0258481,\n",
       " 0.03232407,\n",
       " -0.0040849266,\n",
       " -0.02529732,\n",
       " 0.009628764,\n",
       " 0.013563334,\n",
       " -0.019531528,\n",
       " -0.013340517,\n",
       " 0.023357563,\n",
       " 0.004256978,\n",
       " -0.02207059,\n",
       " -0.012030564,\n",
       " -0.007078964,\n",
       " -0.011205542,\n",
       " 0.009663348,\n",
       " 0.01812374,\n",
       " 0.04316707,\n",
       " -0.017873311,\n",
       " -0.020559417,\n",
       " -0.11768281,\n",
       " -0.01237542,\n",
       " 0.01720002,\n",
       " -0.007628761,\n",
       " 0.0042379196,\n",
       " -0.021176556,\n",
       " 0.01683654,\n",
       " 0.03384142,\n",
       " 0.0093398765,\n",
       " -0.019351136,\n",
       " 0.0062238746,\n",
       " -0.01000337,\n",
       " -0.02046552,\n",
       " 0.00912544,\n",
       " 0.0011650717,\n",
       " 0.11222557,\n",
       " 0.012068661,\n",
       " 0.013664649,\n",
       " 0.0037538928,\n",
       " -0.01557068,\n",
       " -0.015341991,\n",
       " 0.0075632515,\n",
       " -0.014426482,\n",
       " 0.02531103,\n",
       " -0.02471687,\n",
       " 0.0003245681,\n",
       " 0.0073785745,\n",
       " 0.0048739403,\n",
       " 0.0105140265,\n",
       " -0.014644328,\n",
       " -0.0069491477,\n",
       " -0.005943664,\n",
       " 0.0125085125,\n",
       " 0.0007727349,\n",
       " 0.033652853,\n",
       " 0.032846037,\n",
       " 0.010400905,\n",
       " 0.012308818,\n",
       " 0.0043241344,\n",
       " -0.011127168,\n",
       " 0.035724346,\n",
       " -0.012786478,\n",
       " -0.039162513,\n",
       " -0.000115541654,\n",
       " 0.023402024,\n",
       " -0.0077806977,\n",
       " -0.014487559,\n",
       " -0.019780546,\n",
       " 0.010317199,\n",
       " -0.012602136,\n",
       " -0.00803119,\n",
       " -0.055906188,\n",
       " 0.018464582,\n",
       " 0.03402927,\n",
       " -0.03182034,\n",
       " -0.0095554795,\n",
       " 0.017756931,\n",
       " -0.0069185114,\n",
       " 0.0052811275,\n",
       " -0.0063282275,\n",
       " -0.008324774,\n",
       " -0.0054784166,\n",
       " -0.014284419,\n",
       " 0.022153158,\n",
       " 0.013504619,\n",
       " 0.021088963,\n",
       " 0.035744675,\n",
       " -0.019353079,\n",
       " -0.0158639,\n",
       " 0.037478432,\n",
       " 0.0057882774,\n",
       " 0.0071648643,\n",
       " 0.0022143563,\n",
       " -0.008523172,\n",
       " 0.010291568,\n",
       " -0.011792921,\n",
       " 0.0025267315,\n",
       " -0.005213624,\n",
       " -0.003118228,\n",
       " -0.003545531,\n",
       " -0.017286347,\n",
       " -0.004403151,\n",
       " 0.002694821,\n",
       " 0.01591974,\n",
       " 0.005531925,\n",
       " -0.0072631636,\n",
       " -0.031273372,\n",
       " 0.0152001325,\n",
       " -0.036828864,\n",
       " 0.015928471,\n",
       " 0.0037361847,\n",
       " 0.00030893157,\n",
       " 0.00762591,\n",
       " 0.010017235,\n",
       " -0.016203485,\n",
       " -0.027153756,\n",
       " 0.025969656,\n",
       " 0.01054391,\n",
       " 0.018225819,\n",
       " 0.01360083,\n",
       " 0.0039796648,\n",
       " -0.022867026,\n",
       " -0.0043572905,\n",
       " -0.0082417065,\n",
       " 0.012708924,\n",
       " -0.019889086,\n",
       " 0.007470508,\n",
       " -0.0014573742,\n",
       " 0.015672395,\n",
       " 0.0017149035,\n",
       " 0.00073121884,\n",
       " 0.0039269435,\n",
       " -0.0060921717,\n",
       " 0.0069628954,\n",
       " -0.0046116207,\n",
       " 0.0118981395,\n",
       " 0.005628757,\n",
       " -0.0017568301,\n",
       " -0.007974742,\n",
       " 0.003791501,\n",
       " 0.0033565224,\n",
       " 0.020571107,\n",
       " -0.017837964,\n",
       " -6.503459e-05,\n",
       " -0.009865156,\n",
       " -0.027334893,\n",
       " -0.0050453367,\n",
       " 0.0037221252,\n",
       " 0.0073208935,\n",
       " -0.041684315,\n",
       " -0.0012337752,\n",
       " 0.0015978691,\n",
       " 0.008523775,\n",
       " -0.018231506,\n",
       " -0.004133585,\n",
       " -0.008526979,\n",
       " 0.017764712,\n",
       " -0.008105263,\n",
       " 0.0071071032,\n",
       " -0.005085364,\n",
       " -0.009742332,\n",
       " 0.0015289693,\n",
       " -0.0072414386,\n",
       " 0.017925916,\n",
       " 0.013275068,\n",
       " -0.0138047,\n",
       " -0.001511412,\n",
       " -0.009708434,\n",
       " 0.0014221527,\n",
       " 0.019224107,\n",
       " 0.0104600005,\n",
       " -0.021036433,\n",
       " 0.003210795,\n",
       " -0.016955072,\n",
       " -0.009948337,\n",
       " 0.0016374569,\n",
       " -0.005275968,\n",
       " 0.016392447,\n",
       " 0.0075787194,\n",
       " -0.010860746,\n",
       " -0.016373403,\n",
       " 0.013936788,\n",
       " 0.005061254,\n",
       " 0.010101266,\n",
       " 0.015005914,\n",
       " -0.028390944,\n",
       " -0.008282996,\n",
       " -0.023778975,\n",
       " -0.013620249,\n",
       " 0.0073300316,\n",
       " 0.0031813115,\n",
       " 0.012756738,\n",
       " 0.0061715045,\n",
       " 0.01364132,\n",
       " 0.0036568027,\n",
       " 0.01300286,\n",
       " -0.00293748,\n",
       " -0.02304909,\n",
       " 0.0031098868,\n",
       " 0.015285177,\n",
       " 0.007992993,\n",
       " 0.0021765789,\n",
       " -0.011501492,\n",
       " 0.014146046,\n",
       " -0.0048835063,\n",
       " 0.009237472,\n",
       " 0.008871859,\n",
       " 0.007195652,\n",
       " 0.02589373,\n",
       " 0.0033887706,\n",
       " -0.0051098797,\n",
       " -0.009649187,\n",
       " -0.01092538,\n",
       " 0.00072798197,\n",
       " 0.0104131,\n",
       " 0.0042350357,\n",
       " 0.013047866,\n",
       " -0.029744983,\n",
       " 0.0019161578,\n",
       " 0.007924768,\n",
       " 0.014343681,\n",
       " 0.01778756,\n",
       " 0.013551599,\n",
       " 0.002204479,\n",
       " 0.009720311,\n",
       " 0.009710463,\n",
       " 0.0017110404,\n",
       " 0.0112256855,\n",
       " -0.0014512518,\n",
       " 0.014286451,\n",
       " -0.009142006,\n",
       " 0.008253247,\n",
       " 0.009535737,\n",
       " -0.009062027,\n",
       " -0.0082052285,\n",
       " -0.0013904824,\n",
       " -0.00082223053,\n",
       " 0.007395938,\n",
       " 0.0037033618,\n",
       " -0.0063444246,\n",
       " 0.011247669,\n",
       " 0.009753311,\n",
       " 0.018501809,\n",
       " -0.005472752,\n",
       " -0.016179271,\n",
       " -0.0014938951,\n",
       " 0.011311708,\n",
       " 0.0099791,\n",
       " -0.006994755,\n",
       " -0.020981655,\n",
       " 0.007489718,\n",
       " -0.010110547,\n",
       " -0.0033106117,\n",
       " 0.00033162005,\n",
       " 0.009082033,\n",
       " -0.001354889,\n",
       " 0.008811986,\n",
       " -0.009730952,\n",
       " -0.009654886,\n",
       " 0.0025608775,\n",
       " 0.0069464003,\n",
       " 0.0019007452,\n",
       " 0.009520141,\n",
       " -0.0016759617,\n",
       " -0.0044405693,\n",
       " 0.0046820105,\n",
       " 0.01144517,\n",
       " 0.0043173954,\n",
       " -0.012560843,\n",
       " 0.0067644333,\n",
       " 0.0032512774,\n",
       " -0.006878123,\n",
       " -0.015188612,\n",
       " 0.01992262,\n",
       " -0.0018758598,\n",
       " 0.004726363,\n",
       " -0.0029387726,\n",
       " 0.018317318,\n",
       " -0.0064565376,\n",
       " 0.015712332,\n",
       " 0.01624656,\n",
       " -0.005331923,\n",
       " 0.010660162,\n",
       " -0.01699564,\n",
       " 0.006461072,\n",
       " -0.0138479285,\n",
       " 0.008655534,\n",
       " -0.0056832577,\n",
       " -0.015759092,\n",
       " -8.082055e-05,\n",
       " 0.024993733,\n",
       " -0.0053042565,\n",
       " 0.013204308,\n",
       " -0.0036326314,\n",
       " 0.010614809,\n",
       " 0.0018399875,\n",
       " 0.005911156,\n",
       " 0.008623801,\n",
       " -0.021319503,\n",
       " -0.007195126,\n",
       " -0.019430026,\n",
       " -0.011302455,\n",
       " -0.009682825,\n",
       " 0.0101097,\n",
       " 0.012943812,\n",
       " -0.012621727,\n",
       " -0.003467759,\n",
       " 0.0046555926,\n",
       " -0.013162092,\n",
       " -0.013129222,\n",
       " -0.001745727,\n",
       " -0.006646959,\n",
       " -0.01011851,\n",
       " 0.020723011,\n",
       " -0.007005987,\n",
       " 0.0025800297,\n",
       " -0.015466887,\n",
       " -0.015134534,\n",
       " 0.005508143,\n",
       " -0.008008751,\n",
       " -0.008493331,\n",
       " 0.010212207,\n",
       " -0.011497736,\n",
       " 0.002445242,\n",
       " 0.018474096,\n",
       " -0.006547684,\n",
       " -0.0033311401,\n",
       " 0.027637394,\n",
       " 0.007989256,\n",
       " 0.020352026,\n",
       " 0.09298884,\n",
       " 0.0033829066,\n",
       " 0.000749182,\n",
       " -0.0015028794,\n",
       " 0.010164143,\n",
       " 0.0071563576,\n",
       " 0.008796387,\n",
       " -0.00492422,\n",
       " 0.0071439813,\n",
       " -0.005223756,\n",
       " 0.0007834671,\n",
       " -0.012219468,\n",
       " 0.00953163,\n",
       " 0.0068630907,\n",
       " -0.0070761815,\n",
       " -0.0025655674,\n",
       " -0.014660676,\n",
       " 0.0036430703,\n",
       " -0.0135107925,\n",
       " -0.007642413,\n",
       " -0.0027855444,\n",
       " -0.0059061684,\n",
       " -0.0009279147,\n",
       " 0.00020404467,\n",
       " -0.0026541706,\n",
       " 0.0101264175,\n",
       " -0.0040079285,\n",
       " 0.00061997183,\n",
       " 0.00033024713,\n",
       " 0.0036488934,\n",
       " 0.016479557,\n",
       " -0.01065954,\n",
       " ...]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_model.embed_query(\"what is the capital of France?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8397c10d",
   "metadata": {},
   "source": [
    "## Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff3412bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader  ### newer langchain version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ca814de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "857055b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c546f79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\GITHUB Projects\\\\Document_portal\\\\notebook'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b7b4d960",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path=os.path.join(os.getcwd(), \"data\", \"mind the metrics.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "86d623a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader=PyPDFLoader(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d9f461c",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents=loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9f30aff1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Vincent Koc; Jacques Verre; Douglas Blank; Abigail Morgan', 'doi': 'https://doi.org/10.48550/arXiv.2506.11019', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Mind the Metrics: Patterns for Telemetry-Aware In-IDE AI Application Development using the Model Context Protocol (MCP)', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.11019v1', 'source': 'd:\\\\GITHUB Projects\\\\Document_portal\\\\notebook\\\\data\\\\mind the metrics.pdf', 'total_pages': 16, 'page': 0, 'page_label': '1'}, page_content='arXiv:2506.11019v1  [cs.SE]  14 May 2025\\nMind the Metrics: Patterns for Telemetry-Aware In-IDE\\nAI Application Development using Model Context Protocol\\n(MCP)\\nVincent Koc iD vincentk@comet.com\\nJacques Verre jacques@comet.com\\nDouglas Blank iD doug@comet.com\\nAbigail Morgan abigailm@comet.com\\nComet ML, Inc.\\nAbstract\\nModern AI-driven development environments are destined to evolve into observability-first\\nplatforms by integrating real-time telemetry and feedback loops directly into the developer\\nworkflow. This paper introduces telemetry-aware IDEs driven by Model Context Proto-\\ncol (MCP), a new paradigm for building software. We articulate how an IDE (integrated\\ndevelopment environment), enhanced with an MCP client/server, can unify prompt engi-\\nneering with live metrics, traces, and evaluations to enable iterative optimization and robust\\nmonitoring. We present a progression of design patterns: from local large language model\\n(LLM) coding with immediate metrics feedback, to continuous integration (CI) pipelines\\nthat automatically refine prompts, to autonomous agents that monitor and adapt prompts\\nbased on telemetry. Instead of focusing on any single optimizer, we emphasize a general\\narchitecture (exemplified by the Model Context Protocol and illustrated through Comet’s\\nOpik MCP server implementation) that consolidates prompt and agent telemetry for the\\nfuture integration of various optimization techniques. We survey related work in prompt\\nengineering, AI observability, and optimization (e.g., Prompts-as-Programs, DSPy’s MIPRO,\\nMicrosoft’s PromptWizard) to position this approach within the emerging AI developer\\nexperience. This theoretical systems perspective highlights new design affordances and\\nworkflows for AI-first software development, laying a foundation for future benchmarking\\nand empirical studies on optimization in these environments.\\n1 Introduction\\nThe rise of large language models and AI “copilot” systems has transformed software development, introducing\\nAI agents into Integrated Development Environments (IDEs) to assist with coding, content generation, and\\ndecision-making (Hou et al., 2025). However, developing AI-driven applications poses new challenges: LLM-\\nbased components are non-deterministic, difficult to debug, and often behave in a non-transparent (black-box)\\nmanner. Traditional software engineering practices rely on **observability** (the ability to infer internal\\nstates of a system from its external outputs like logs, metrics, and traces) to understand and debug systems,\\nand a specialized discipline, LLMOps (Large Language Model Operations), is emerging to address the unique\\nlifecycle management challenges of LLMs (Management Solutions, 2023; Shi et al., 2024). We propose that\\nAI-first IDEs should similarly be telemetry-aware, treating prompts and AI agent interactions with the\\nsame rigor as code, by integrating real-time **telemetry** (the collection and transmission of data such\\nas evaluation metrics, trace logs, and performance signals from remote or internal system components for\\nmonitoring) into the development loop, a concept we term “Agent-Integrated Development Environment\\n(AIDE)”.\\n1'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Vincent Koc; Jacques Verre; Douglas Blank; Abigail Morgan', 'doi': 'https://doi.org/10.48550/arXiv.2506.11019', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Mind the Metrics: Patterns for Telemetry-Aware In-IDE AI Application Development using the Model Context Protocol (MCP)', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.11019v1', 'source': 'd:\\\\GITHUB Projects\\\\Document_portal\\\\notebook\\\\data\\\\mind the metrics.pdf', 'total_pages': 16, 'page': 1, 'page_label': '2'}, page_content='Developer’s AI\\nApplication\\nOpik\\nTelemetry & Trace Store\\nOpik MCP Client\\n(HTTP / gRPC)\\nAI-Assisted IDE\\n(Tool-calling LLM)\\ntrace stream\\ngetMetrics()\\nquery metricsmetrics / traces\\ntelemetry context\\nFigure 1: Baseline telemetry workflow with the Model Context Protocol (MCP). The AI application streams\\ntraces directly into Opik. An MCP client retrieves metrics/traces from Opik on demand. The AI-assisted\\nIDE uses telemetry signals from the MCP client (via the diagonal dashed arrow), feeding its tool-calling\\nLLM to refine prompts and code.\\nAI observability has recently emerged as a critical need for reliable deployment of LLM applications (Sergeyuk\\net al., 2024). Observability provides insight into an AI system’s behavior by gathering runtime data (or\\ntelemetry) such as prompts, model responses, cost, and quality metrics. New platforms have been built to log\\nand analyze LLMs, allowing teams to debug complex prompt chains, measure accuracy and hallucination\\nrates, and assess how changes impact model outputs (Comet, 2025a). Yet, these capabilities are often external\\nto the IDE and continuous integration process. We argue for observability-first design: making telemetry and\\nevaluations a first-class citizen within the IDE, so that developers (and the AI) can iterate on prompts and\\nagent desgin with faster feedback, analogous to how they iteratively refine code using logs and tests.\\nThis paper introduces telemetry awareness to an IDE to facilitate AI developement. This is inspired by the\\nrecently proposed Model Context Protocol (MCP), a standardized open-interface for connecting AI models\\nwith external tools and resources (Hou et al., 2025; Anthropic, 2024). In our vision, an MCP client/server\\nacts as a unified broker of metrics (quantitative signals and traces), control (commands or adjustments to AI\\nagents), and prompt data (prompt templates and versions). By interfacing the IDE with an MCP server,\\ndevelopers can seamlessly manage prompts, retrieve evaluation metrics, inspect traces of the LLM’s reasoning,\\nand could even send control instructions to running agents in a consistent and standardized manner. This\\ntransforms the IDE into an interactive AI observability dashboard as well as a coding environment.\\nTo ground this paradigm, we describe a progression of design patterns that illustrate increasing integration of\\ntelemetry in the AI development workflow:\\n• Local Development with Metrics-in-the-Loop: Developers working in an AI-enhanced IDE receive\\nreal-time feedback from the LLM runs in the form of metrics and traces (including evaluations). The\\nIDE can display evaluation results (e.g. error rates, token usage, latency, accuracy, hallucination)\\nafter each prompt or application execution and allow querying of trace logs. This immediate feedback\\nloop helps identify flaws in prompts or agent logic early on.\\n• CI-Integrated Prompt Optimization: Telemetry-aware design extends to continuous integration.\\nPrompt quality checks and automated optimizations run as part of CI pipelines, using stored traces\\nand metrics to refine prompts or catch regressions. The MCP protocol provides a consistent API to\\nfetch evaluation results and update prompts programmatically during testing and QA phases.\\n• Autonomous Monitoring Agents: In the most advanced pattern, autonomous monitoring agents lever-\\nage the telemetry stream to continuously watch an AI system in runtime and suggest improvements.\\nThese agents, which can be implemented as LLM-based evaluators or scripts, use the MCP interface\\nto query interactions and metrics, then generate improved prompts or fine-tuning instructions without\\ndirect human intervention.\\n2'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Vincent Koc; Jacques Verre; Douglas Blank; Abigail Morgan', 'doi': 'https://doi.org/10.48550/arXiv.2506.11019', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Mind the Metrics: Patterns for Telemetry-Aware In-IDE AI Application Development using the Model Context Protocol (MCP)', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.11019v1', 'source': 'd:\\\\GITHUB Projects\\\\Document_portal\\\\notebook\\\\data\\\\mind the metrics.pdf', 'total_pages': 16, 'page': 2, 'page_label': '3'}, page_content='Crucially, we emphasize the architecture that makes these patterns possible. We detail how an open-source\\nMCP client/server can unify prompt and agent telemetry across development and deployment environments,\\nenabling future integration of optimizers. Rather than proposing a specific optimization algorithm, our focus\\nis on the design affordances of this architecture, specifically how it creates a foundation upon which tools like\\nDSPy’s MIPRO or PromptWizard could plug in to automatically tune prompts using the rich data collected.\\nBy decoupling the collection of metrics, traces and evaluations from the optimization logic, the MCP-based\\napproach allows any number of optimization strategies (heuristic, learned, or hybrid) to be applied in a\\nmodular way.\\nThis work is a theoretical and architectural insight paper. We do not present benchmark experiments or\\nclaim specific performance improvements. Instead, we identify emerging workflows and frameworks that\\npoint toward a new developer experience for AI applications. We cite related efforts in prompt engineering\\nand LLM observability to show how the community is converging on this paradigm. Our hope is that this\\nconceptual framework will inform and inspire more comprehensive evaluations in future work, where the\\nimpact of telemetry-aware development on model performance, developer productivity, and reliability can be\\nsystematically measured. However, we have made early progress on developing a prototype MCP client/server,\\napplied to test-cases and have started on experimentation with IDE-assisted prompt optimization.\\nThe rest of this paper is organized as follows. Section 2 reviews related work in AI-assisted IDEs, prompt\\noptimization, and observability for LLM systems. Section 3 introduces the telemetry-aware design patterns\\nin detail, illustrating each stage of the development lifecycle. Section 4 presents the MCP architecture and\\nits implementation in an example system, describing how it unifies metrics, control signals, and prompts.\\nSection 5 discusses the implications of this paradigm and outlines avenues for future research, including the\\nintegration of advanced optimizers. Finally, Section 6 concludes with a summary of key insights. An appendix\\nprovides supplementary materials and figures.\\n2 Background and Related Work\\n2.1 AI Integration in Development Environments\\nIntegrated Development Environments (IDEs) have begun incorporating AI assistance to enhance programmer\\nproductivity and decision-making (Hou et al., 2025). For example, code completion and synthesis tools\\npowered by LLMs (e.g., GitHub Copilot, Cursor, Zencoder’s agentic solutions (Zencoder.ai, 2024)) have\\nbecome common. Recent studies survey the Human-AI experience in IDEs, highlighting the need to design\\nappropriate user interfaces and interactions for these AI features (Wang et al., 2022). Key challenges include\\nbuilding trust in AI suggestions, ensuring readability of AI-generated code, and designing task-specific UI\\naffordances for effective collaboration between the developer and the AI assistant (Anuyah et al., 2023; Wang\\net al., 2022). These findings underscore that integrating AI into development is not just a matter of model\\nquality, but also of how information flows to the developer. Our work builds on this premise by channeling rich\\ntelemetry information into the IDE, thus giving developers deeper insight into the AI’s behavior (addressing\\ntrust and transparency) and enabling more informed interventions.\\nAnother trend is the emergence of frameworks where LLMs act as controllers or orchestrators in complex\\nworkflows. HuggingGPT is a prominent example, where an LLM (ChatGPT) manages calls to numerous\\ntask-specific models to solve multi-step AI tasks. In HuggingGPT, the LLM plans a sequence of subtasks,\\nselects appropriate models from a model hub (HuggingFace), invokes them, and aggregates the results. This\\ndemonstrates an agentic use of LLMs that goes beyond single-turn prompting; the LLM essentially becomes\\na runtime decision-maker coordinating a pipeline. Such multi-component “AI programs” are powerful but\\ndifficult to debug or optimize, as errors can arise from prompt mis-specifications, model selection issues, or data\\npassing between steps. This motivates first-class observability for LLM-driven workflows. By instrumenting\\neach step (e.g., logging model outputs, timestamps, intermediate prompts), developers can trace the chain of\\nevents and identify bottlenecks or failure points. Our proposed MCP-driven approach aligns with this need,\\nas it provides a uniform way to capture and query traces from multi-agent or tool-augmented LLM systems.\\nIn fact, the value of capturing trace data for agent-based systems is already being recognized. For instance,\\nComet’s Opik platform can record full execution traces of agent frameworks like HuggingGPT or Google’s\\n3'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Vincent Koc; Jacques Verre; Douglas Blank; Abigail Morgan', 'doi': 'https://doi.org/10.48550/arXiv.2506.11019', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Mind the Metrics: Patterns for Telemetry-Aware In-IDE AI Application Development using the Model Context Protocol (MCP)', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.11019v1', 'source': 'd:\\\\GITHUB Projects\\\\Document_portal\\\\notebook\\\\data\\\\mind the metrics.pdf', 'total_pages': 16, 'page': 3, 'page_label': '4'}, page_content='Agent Development Kit, giving developers “deep visibility into agent behavior and orchestration” (Comet,\\n2025a).\\n2.2 Prompt Engineering and Optimization Methods\\nPrompt engineering has emerged as a critical discipline for aligning LLM behavior with user needs. Early\\nprompt engineering relied on manual craft and intuition, but as applications scale, there is growing interest\\nin automated prompt optimization. Two notable research directions are: (1) treating prompts as structured,\\nmodular programs, and (2) using AI feedback loops to refine prompts.\\nIn the first category, prompts are viewed not as monolithic strings but as compositions of reusable components\\nlike instructions, examples, and context inserts. The \"Prompts-as-Programs\" concept formalizes this view\\n(Schnabel & Neville, 2024), with Schnabel and Neville (2024) introducing Sammo, a framework for compile-\\ntime prompt optimization. In Sammo, a complex prompt (or \"metaprompt\") is represented as a call graph of\\ncomponents (e.g., functions to render different sections of the prompt). This structured representation allows\\nsystematic transformations akin to compiler optimizations, for example, pruning irrelevant instructions or\\ntuning hyperparameters of prompt components (Schnabel & Neville, 2024). Sammo’s approach was influenced\\nby DSPy (Declarative Self-Improving Python), which also treats prompt engineering as programming. DSPy is\\nan open-source framework that enables developers to define LLM application logic in a modular way and then\\noptimize prompts through code-based experimentation. Rather than relying purely on trial-and-error, DSPy\\nprovides teleprompter algorithms (e.g., Cooperative, Bayesian optimizers) that iteratively adjust prompt\\ninstructions and few-shot examples to improve task performance (Khattab et al., 2023). The advantage\\nof DSPy is that it integrates prompt tracking and tuning into a code workflow: one can programmatically\\nlog model outputs and evaluate them against assertions or examples, then let the optimizer propose better\\nvariants. This aligns closely with telemetry-aware development, where tracking essentially involves collecting\\nmetrics on prompt outcomes, and self-improvement uses these metrics to drive prompt updates.\\nIn the second category, AI feedback loops, we see techniques like Microsoft’s PromptWizard (Agarwal et\\nal., 2024). PromptWizard is a fully automated framework where the LLM itself is employed to critique and\\nrefine prompts in an iterative loop. The process begins with an initial prompt (and possibly a few training\\nexamples) and proceeds as follows: the LLM generates multiple candidate prompt variants, tests them (by\\ntrying to solve the task or by using an evaluation function), and then analyzes its own outputs to suggest\\nimprovements. This “self-evolving” mechanism means the LLM alternates between being a solver and a critic,\\nusing each round’s feedback to produce a better prompt in the next round. PromptWizard jointly optimizes\\nthe instruction part of the prompt and the choice of few-shot examples, even synthesizing new examples that\\nexpose weaknesses in the prompt. Notably, PromptWizard’s design reflects a general template: provide a\\nfeedback signal, and let the model itself search for prompt improvements. The feedback can be as simple as\\ntask accuracy or a critique rubric, and the search is guided by the model’s ability to introspect on errors. In\\nessence, this approach also leverages telemetry (feedback metrics) to drive prompt updates, although the\\n’agent’ acting on telemetry is the LLM itself.\\nThe above approaches (Sammo, DSPy, PromptWizard) are complementary to our proposed paradigm. They\\nsupply algorithms and frameworks for optimizing prompts, while our MCP-based telemetry integration\\nprovides the necessary infrastructure to apply such optimizers continuously and contextually. In particular, an\\nobservability-first IDE could feed actual usage traces and evaluation metrics into a DSPy’s MIPRO optimizer\\nor PromptWizard loop. Prior work often used static datasets or fixed evaluation sets for optimization; by\\ncontrast, an MCP telemetry pipeline could enable optimizers to work with live data from real user interactions\\nand test cases as they happen, closing the gap between prompt tuning and deployment behavior.\\n2.3 Telemetry and Observability for LLM Systems\\nObservability is a well-established concept in software reliability, referring to the ability to infer internal\\nstates of a system from its external outputs such as logs, metrics, and traces. For LLM-based applications,\\ntelemetry data includes prompt inputs, model outputs, intermediate reasoning steps (if available), timings,\\ntoken usage, and user feedback, among other signals. Several recent works and tools underline the importance\\nof LLM observability. On the conceptual side, Onose et al. (2024) define LLM observability as \"the practice\\n4'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Vincent Koc; Jacques Verre; Douglas Blank; Abigail Morgan', 'doi': 'https://doi.org/10.48550/arXiv.2506.11019', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Mind the Metrics: Patterns for Telemetry-Aware In-IDE AI Application Development using the Model Context Protocol (MCP)', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.11019v1', 'source': 'd:\\\\GITHUB Projects\\\\Document_portal\\\\notebook\\\\data\\\\mind the metrics.pdf', 'total_pages': 16, 'page': 4, 'page_label': '5'}, page_content='of gathering data (telemetry) while an LLM-powered system is running to analyze, assess, and enhance\\nits performance\" (Sergeyuk et al., 2024). This involves recording prompts and responses, tracing requests\\nthrough any pipelines or chains, monitoring performance metrics (like latency and error rates), and evaluating\\noutput quality via automated or human feedback. The goal is to provide developers with visibility into the\\nbehavior of otherwise opaque AI components, thereby enabling debugging and iterative improvement.\\nAlongside the growth of LLM observability, the broader field of LLMOps (Large Language Model Operations)\\nhas rapidly evolved as an adaptation of MLOps principles tailored for the specific needs of LLMs (Management\\nSolutions, 2023; Sinha et al., 2024). LLMOps encompasses the entire lifecycle of LLMs, from data preparation\\nand model training through deployment, monitoring, and maintenance in production environments (Manage-\\nment Solutions, 2023; Pahune & Akhtar, 2025). This discipline addresses significant challenges inherent to\\nlarge models, such as managing vast datasets, scaling computational resources (often requiring specialized\\nhardware like GPUs/TPUs and parallelization techniques), and ensuring continuous model performance,\\nwhich includes mitigating biases, detecting hallucinations, and countering model degradation over time (Man-\\nagement Solutions, 2023; Shi et al., 2024). Key best practices in LLMOps include robust data management,\\ncontinuous model training and fine-tuning, designing scalable infrastructure, rigorous monitoring and observ-\\nability, security and compliance, inference optimization, CI/CD for seamless updates, collaborative workflows,\\nhuman-in-the-loop (HITL) mechanisms for quality control, and adherence to ethical considerations (Pahune\\n& Akhtar, 2025). LLMOps emphasizes ensuring the stability, reliability, interpretability, and maintainability\\nof models, often requiring real-time monitoring and proactive strategies to address issues promptly (Shi et\\nal., 2024). Recent advancements further highlight the importance of HITL systems, adversarial testing for\\nrobustness, and mature CI/CD pipelines to enhance LLM accuracy and ensure scalable, reliable deployments\\n(Pahune & Akhtar, 2025).\\nMultiple platforms have arisen to support LLM observability in practice, including Opik by Comet and other\\ntools built on open standards such as OpenTelemetry. These platforms typically offer SDKs or APIs to\\ninstrument LLM calls in applications, sending trace data to a centralized dashboard. Common features are\\nstoring all prompt inputs and outputs, logging metadata (timestamps, model versions, prompt templates\\nused), and attaching evaluations (e.g., automatically scoring model responses for correctness or toxicity via\\nclassifier models or LLM judges). For instance, Opik (an open-source LLM evaluation platform) allows teams\\nto “log, evaluate, and iterate” on LLM prompts and agent flows with a scalable backend (Comet, 2025a). A\\ndistinctive aspect of Opik is its focus on tracing and evaluation as core capabilities: it records the sequence of\\ncalls in an agent or chain, and it can run evaluation routines on each output (such as computing hallucination\\nscores or comparing against reference answers). The platform must handle high-volume data from every LLM\\ninteraction and support analytical queries over this data. Technologies like OpenTelemetry, a standard for\\ncapturing and exporting trace data, have been extended to work with LLM-specific data, allowing integration\\nwith existing APM (Application Performance Monitoring) tools.\\nThe Model Context Protocol (MCP) has recently been proposed as a standardized way to integrate such\\nobservability and control across AI tools. Hou et al. (2025) introduce MCP as “a standardized interface\\ndesigned to enable seamless interaction between AI models and external tools and resources, breaking\\ndown data silos and facilitating interoperability across diverse systems” (Hou et al., 2025). Anthropic also\\nopen-sourced their version of the Model Context Protocol in late 2024, aiming to create a universal standard\\nfor connecting AI systems with diverse data sources, thereby simplifying how AI systems access necessary\\ndata (Anthropic, 2024). Their work outlines the core components and workflow of MCP, including the concept\\nof MCP servers that manage the context for AI models (prompts, tools, state) through well-defined phases\\nlike creation, operation, and update. They also highlight industry adoption and use cases of MCP, indicating\\nthat companies are beginning to build infrastructure around this protocol for AI orchestration. Our use of\\nthe term MCP aligns with this vision, but we emphasize the “Metrics, Control, Prompt” interpretation as it\\nrelates to observability. In our telemetry-aware context, an MCP server is essentially an observability and\\ncontrol hub. It exposes APIs to log and query metrics/traces, to manage and version prompts, and to send\\ncontrol commands (for instance, instructing an agent to switch tools or trigger a reset) in a unified manner.\\nThe next section will illustrate how this MCP-based design underpins new development workflows.\\n5'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Vincent Koc; Jacques Verre; Douglas Blank; Abigail Morgan', 'doi': 'https://doi.org/10.48550/arXiv.2506.11019', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Mind the Metrics: Patterns for Telemetry-Aware In-IDE AI Application Development using the Model Context Protocol (MCP)', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.11019v1', 'source': 'd:\\\\GITHUB Projects\\\\Document_portal\\\\notebook\\\\data\\\\mind the metrics.pdf', 'total_pages': 16, 'page': 5, 'page_label': '6'}, page_content='Edit Prompt\\n(CLI/Notebook)\\nRun Models\\n& Collect Logs\\nInspect Logs\\n& Metrics\\nTweak Prompt\\n& Repeat\\n1 2 3\\n4\\nEdit Prompt\\n(In-IDE)\\nRun in IDE\\n+ Auto-Telemetry\\nAI-Suggested\\nPrompt Tweak\\n1 2\\n3\\nTraditional Human Development Loop\\nTelemetry-Enhanced IDE Development Loop\\nUser\\nFigure 2: Comparison between traditional prompt-engineering workflow (top) and MCP-enhanced IDE\\ndevelopment loop (bottom). The traditional approach requires manual context switching between tools, while\\nthe telemetry-aware IDE integrates all steps within a unified environment, providing real-time feedback and\\nAI-assisted prompt optimization.\\n3 Telemetry-Aware Design Patterns in AI Development\\nIn this section, we outline three progressive design patterns for AI application development, each incorporating\\ntelemetry and feedback at increasing levels of automation. While various generative AI design patterns\\nhave been proposed (Koc, 2024), the feedback loop and direct interaction mechanisms for developers within\\nmany of these emerging LLM patterns remain an area requiring further clarification and development. These\\npatterns are (1) Local metrics-in-the-loop coding, (2) CI-integrated prompt optimization, and (3) Autonomous\\nmonitoring agents. They correspond to stages in the development and deployment lifecycle, but the underlying\\nprinciples are similar: use observability data to drive improvements in prompts or agent policies. We describe\\neach pattern and provide examples of how an MCP-enabled IDE could facilitate the workflow.\\n3.1 1. Local Development with Metrics in the Loop\\nThe first pattern involves the individual developer working within an IDE that provides real-time feedback\\nabout the AI’s behavior. In a standard IDE, a developer writing code might rely on debug logs or a watch\\nwindow to inspect program state after running the code. By analogy, in a telemetry-aware IDE, whenever\\nthe developer runs an AI-powered function (e.g., executes a prompt or tests an agent in a sandbox), the\\nenvironment immediately surfaces telemetry insights: summary metrics and trace excerpts from that run.\\nConcretely, consider a developer iterating on a prompt for a question-answering agent. They write an initial\\nprompt and execute it with some test query. A telemetry-aware IDE could automatically display information\\nsuch as: the number of tokens in the prompt vs. completion, the model’s response latency, and any evaluation\\nresults (e.g., whether the answer matches a ground truth if available, or a rating from a critique model).\\nMoreover, the developer could inspect the trace of the agent’s reasoning if the agent uses a chain-of-thought\\nor tool use. For example, if the agent is supposed to call an API and then answer, the trace might show what\\nAPI call was made and the result, enabling the developer to verify that the agent followed the correct steps.\\nThe MCP integration allows the developer to ask the IDE (or rather, the LLM assistant in the IDE) questions\\nabout the telemetry: e.g. \"From your trace logs, what are common sources of error?\" or \"Suggest prompt\\nimprovements based on the last 10 interactions.\" (An illustrative example of the IDE displaying telemetry\\ninsights and providing a prompt recommendation is shown in Figure 5 located in Appendix A). The IDE\\n6'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Vincent Koc; Jacques Verre; Douglas Blank; Abigail Morgan', 'doi': 'https://doi.org/10.48550/arXiv.2506.11019', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Mind the Metrics: Patterns for Telemetry-Aware In-IDE AI Application Development using the Model Context Protocol (MCP)', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.11019v1', 'source': 'd:\\\\GITHUB Projects\\\\Document_portal\\\\notebook\\\\data\\\\mind the metrics.pdf', 'total_pages': 16, 'page': 6, 'page_label': '7'}, page_content='forwards such queries to the MCP server, which has access to the stored traces and metrics of the project,\\nand the LLM then composes an answer by analyzing that data. In the illustrated case (see Appendix A),\\nthe assistant identified that the conversation flow was slow (long response times) and users often provided\\ncertain inputs, then recommended adding explicit instructions in the prompt to structure those inputs. This\\nkind of iterative loop – trial prompt→ run → telemetry → revised prompt – can be done manually (with\\nthe developer reading metrics and adjusting) or in a human-in-the-loop manner (with the LLM suggesting\\nadjustments based on telemetry and the developer accepting or tweaking them).\\nThe benefit of local metrics-in-the-loop development is rapid feedback. Prompt engineering traditionally\\nhas a slow feedback cycle: one runs a prompt on a few examples and subjectively judges the outputs. With\\nintegrated telemetry, the feedback becomes more objective and immediate, as the system can highlight\\nquantitative issues (e.g., \"The last 5 outputs had an average hallucination score of 0.7, which is high\") and\\neven point to specific trace examples that illustrate a problem. This accelerates the prompt refinement\\nprocess. It also engenders a mindset shift – prompts and agent behaviors are not \"magic\" or static; they\\nproduce data that can be inspected and responded to. In essence, the developer is debugging prompts with\\nlogs, much as they would debug code with print statements.\\nEnabling this pattern requires the IDE to have access to a backend that accumulates and indexes telemetry\\nfor queries. The MCP client/server fulfills this role, acting as a repository of all prompts executed and their\\noutcomes. A developer can retrieve, for instance, \"the most recent trace with a high hallucination score\"\\nor \"the output of the last conversation turn\", via natural language queries that an MCP-enabled system\\ninterprets (Comet, 2025a;b). The IDE plugin translates these queries (possibly using a special syntax or an\\nAPI call) to MCP requests. Under the hood, the server might store traces in a database and maintain metrics\\nlike token counts and evaluation scores per trace. When queried, it can filter and aggregate this information.\\nThe results are then fed back into the IDE’s LLM assistant to generate a human-readable summary or\\nrecommendation. This seamless loop is what makes the observability-first IDE experience powerful: the\\ndeveloper is essentially conversing with both the LLM and the data about the LLM’s past performance, all\\nwithin one interface.\\n3.2 2. Continuous Integration with Telemetry-Guided Optimization\\nThe second pattern extends telemetry-aware practices into the CI/CD (Continuous Integration/Continuous\\nDeployment) pipeline. In modern software projects, CI is used to automatically run test suites, linters,\\nand other analysis on every code change, catching regressions early. We propose that AI-centric projects\\nincorporate prompt and agent evaluations in CI, enabled by the same telemetry infrastructure used in\\ndevelopment.\\nA concrete example is integrating a suite of prompt test cases: for instance, a set of input queries with\\nexpected answer characteristics (not necessarily one correct answer, but perhaps certain constraints like\\n“should cite a source” or “should not contain profanity”). Whenever a developer updates a prompt or changes\\nthe agent chain, the CI can run these test queries through the system (maybe using a fixed LLM checkpoint\\nfor consistency) and log the outputs to the MCP server. The MCP server, in turn, evaluates the outputs\\nagainst the expectations using predefined evaluators or metrics (these could be simple regex checks, or more\\nadvanced LLM-based evaluators for coherence, etc.). The resulting metrics (like “pass/fail” counts for each\\ntest, or average scores) are then used to determine if the change introduced a regression. This is analogous\\nto unit tests for code – here we have unit tests for prompts/agents. If a significant drop in performance is\\ndetected (say the average relevance score dropped by 10%), the CI can flag the build or even automatically\\nrevert the prompt change. The use of time-series data accessible through basic language queries can empower\\nnon-technical colleagues to define test cases and facilitate regression testing, for example, by monitoring for\\ndrift beyond a certain percentage point.\\nTelemetry plays a central role in this pattern by providing a historical baseline and context for each run.\\nBecause the server accumulates data over time, it can compare the current run’s metrics to previous runs. For\\ninstance, it can answer: “Is the accuracy of the Q&A prompt in this commit within the expected range based\\non the last 10 commits?” If not, something might be wrong. Moreover, storing traces from CI runs means\\nthat when a failure occurs, developers can inspect the exact inputs and outputs that led to it, using the same\\n7'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Vincent Koc; Jacques Verre; Douglas Blank; Abigail Morgan', 'doi': 'https://doi.org/10.48550/arXiv.2506.11019', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Mind the Metrics: Patterns for Telemetry-Aware In-IDE AI Application Development using the Model Context Protocol (MCP)', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.11019v1', 'source': 'd:\\\\GITHUB Projects\\\\Document_portal\\\\notebook\\\\data\\\\mind the metrics.pdf', 'total_pages': 16, 'page': 7, 'page_label': '8'}, page_content='Dev Push\\n(Prompt / Code)\\nCI Job\\n(Test & Telemetry)\\nTelemetry Check\\n(MCP Query)\\ntrigger log\\nAuto-Opt\\n(Optional)\\nPR w/ Suggestions\\n(Human Review) Approve?\\nif regress\\npatch\\ntests passed\\nyes, merge\\nno, refine\\nUser\\ne.g. run only if metrics drop>10%\\nFigure 3: CI/CD pipeline with MCP telemetry, user trigger, and clear approval flow. A user initiates the\\ncommit; telemetry checks drive optional optimization, and an approval decision routes to merge or further\\nrefinement.\\nIDE tooling described in Pattern 1. This tightens the integration between development and testing: rather\\nthan digging through log files on a CI server, a developer could simply query the MCP from their IDE (e.g.,\\n“Show me examples of where the new prompt failed the tests.”) to retrieve the offending traces for review.\\nAnother aspect is automated prompt optimization in CI. Since CI can execute code, it can also execute\\nprompt optimization routines provided by frameworks like DSPy’s MIPRO or PromptWizard (Khattab et al.,\\n2023; Agarwal et al., 2024), as long as they expose an API or script. Imagine a scenario where a developer\\nprovides an initial version of a complex prompt, perhaps by roughly writing out sections and examples.\\nInstead of expecting the developer to manually fine-tune it, the CI could trigger an optimization job that\\nuses, say, DSPy’s MIPRO algorithm or PromptWizard’s self-refinement. This job would use the MCP data\\n(or a static training set) as a basis to evaluate candidate prompts. The optimization might run for a few\\nminutes and then output a refined prompt suggestion. The CI could then either automatically replace the\\nprompt (which is risky and perhaps better for an offline process) or open a merge request for the developer\\nto review the optimized prompt. Essentially, the continuous part of CI would include continuously searching\\nfor better prompts whenever changes occur.\\nThere are early signs of this approach. For example, DSPy’s philosophy is to integrate prompt tuning into\\nthe development pipeline programmatically. One could incorporate a DSPy “program” in the repository that\\ndefines the prompt and an optimization objective; the CI then runs that program to update the prompt.\\nMicrosoft’s PromptWizard, while presented as an interactive research tool, could be adapted to run headless\\nin CI to improve a prompt overnight. The key enabler is having a reliable evaluation metric or suite, which\\nthe telemetry system provides. If the project has accumulated real usage traces and labeled outcomes, those\\ncan serve as a testbed for optimization in CI (which is superior to generic benchmarks because it reflects the\\nactual use case).\\nIn summary, telemetry-aware CI means treating prompt/agent quality as a continuously tested property of\\nthe system. The MCP protocol provides the glue: it ensures that both the IDE and CI jobs speak the same\\nlanguage to log and retrieve prompt executions and their evaluations. A prompt tested in CI with certain\\nmetrics will have those metrics stored via MCP, and the developer can later query or visualize them in the\\nIDE. Conversely, issues discovered during local development (and their fixes) can be codified as CI tests to\\n8'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Vincent Koc; Jacques Verre; Douglas Blank; Abigail Morgan', 'doi': 'https://doi.org/10.48550/arXiv.2506.11019', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Mind the Metrics: Patterns for Telemetry-Aware In-IDE AI Application Development using the Model Context Protocol (MCP)', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.11019v1', 'source': 'd:\\\\GITHUB Projects\\\\Document_portal\\\\notebook\\\\data\\\\mind the metrics.pdf', 'total_pages': 16, 'page': 8, 'page_label': '9'}, page_content='prevent regressions. This synergy leads to a more robust development lifecycle, where improvements and bug\\nfixes in prompts are tracked and validated just like code changes.\\n3.3 3. Autonomous Monitoring and Self-Improvement Agents\\nThe third pattern is forward-looking: it envisions deployed AI systems that include autonomous agents\\nmonitoring other agents. Once an application is running in production (e.g., a deployed chatbot or an\\nagent-based workflow serving users), telemetry can be used not only by human developers but also by\\nautomated “watchers” that ensure the system behaves optimally. We refer to these as autonomous monitor\\nagents.\\nConsider a complex multi-agent system, for instance, a customer support bot that has multiple sub-agents\\nfor different tasks. This system produces a continuous stream of interactions and outcomes. An autonomous\\nmonitor agent could be implemented (possibly as an LLM itself or a traditional program) which subscribes to\\nthis stream via the MCP interface. This agent might periodically query the MCP server for recent traces\\nwith certain characteristics, for example: “find conversations in the last hour where the user was unhappy\\n(negative feedback) or the agent’s answer had a low confidence score.” Given those problematic traces, the\\nmonitor agent could attempt to diagnose the issue. If it’s implemented as an LLM prompt, it might take\\nthose traces and analyze them: “What went wrong in these interactions? Is there a pattern?” Suppose it\\nfinds that many failures involve a particular tool the agent is using incorrectly. The monitor could then\\nformulate a suggested fix, perhaps “When using the database lookup tool, ensure to check for null results,”\\nwhich could translate to a prompt adjustment or a minor code modification in the agent’s logic.\\nA more concrete example involves OpenAI’s function-calling API, which allows tools to be invoked by the\\nmodel. Imagine an agent that uses function calls to retrieve data. A monitor agent could watch function call\\ntraces via MCP and notice if a certain function call often returns errors. It could then autonomously create a\\npull request or a patch to the agent’s prompt, adding an instruction like “If the database query fails, try\\nan alternative query or apologize to the user.\"“ This patch would go through the normal CI process which,\\nthanks to telemetry integration, can evaluate if the patch helps. In a sense, the monitor agent is acting as a\\nspecialized developer that continuously fine-tunes the system using evidence from telemetry. This also means\\nadditional logging and testing may not be required within a given AI application, as existing LLM-specific\\ntelemetry solutions can be leveraged.\\nWhile fully autonomous self-improving systems are still experimental, components of this vision are taking\\nshape. Microsoft’s PromptWizard already demonstrates an LLM refining prompts based on feedback in a\\nloop, which can be seen as a micro-scale “self-healing” mechanism for a prompt. There are also research\\nefforts on meta-prompts where one LLM monitors another’s outputs for correctness or safety, often called\\nan “evaluator” or “judge” model. Our paradigm can incorporate such evaluator models as first-class agents\\nconnected via MCP. For example, a hallucination watchdog LLM could be subscribed to each new answer\\nproduced by the main model. If it detects a likely hallucination (perhaps by cross-checking facts), it could\\ntrigger an intervention, such as logging an alert via MCP or even instructing the main model to provide\\nsources.\\nThe architectural advantage of MCP here is unification and access control. A monitor agent needs a lot of\\ndata to be effective – potentially a feed of all interactions. MCP can provide a filtered stream (via an API or\\nevent subscription) of telemetry data to authorized monitor agents. Because MCP manages prompts and\\ncontext, these monitor agents could also use it to update prompts or settings. This is the Control aspect of\\nMCP: beyond passive observation, the protocol could allow certain clients to make changes (e.g., deploy a\\nnew prompt version or adjust a parameter) in a controlled manner. In practice, such control actions would\\nbe gated by policies or human review for safety. However, even without full autonomy, a monitor agent could\\nat least propose changes. It might open a ticket or notify a developer with a suggested improvement and\\nrelevant evidence from logs.\\nAutonomous monitoring agents lead the ultimate goal of telemetry-aware design: a system that not only\\nobserves itself but improves itself over time (in an evolutionary manner). Achieving this in production reliably\\nwill require advances in ensuring the quality of the agent’s suggestions (so that it doesn’t introduce new\\nproblems) and in governance (to avoid a scenario where an autonomous agent makes inappropriate changes).\\n9'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Vincent Koc; Jacques Verre; Douglas Blank; Abigail Morgan', 'doi': 'https://doi.org/10.48550/arXiv.2506.11019', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Mind the Metrics: Patterns for Telemetry-Aware In-IDE AI Application Development using the Model Context Protocol (MCP)', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.11019v1', 'source': 'd:\\\\GITHUB Projects\\\\Document_portal\\\\notebook\\\\data\\\\mind the metrics.pdf', 'total_pages': 16, 'page': 9, 'page_label': '10'}, page_content='That said, having the MCP infrastructure in place greatly simplifies prototyping such agents, because they\\ncan be developed and tested in the same way any client uses the MCP API. One could develop a monitor\\nagent offline, feed it past telemetry data from MCP (e.g., logs from last week) to see if it correctly identifies\\nissues, and then gradually deploy it to live monitoring. This incremental path is similar to how one might\\ndeploy a new microservice – starting in shadow mode, then giving it more responsibilities. The key difference\\nis that the monitor agent’s domain is the AI’s own behavior.\\n4 Architecture: The MCP Server for Unified Telemetry and Control\\nCentral to the above design patterns is the MCP server – the component that enables IDEs, CI pipelines, and\\nagents to all share a common view of prompts, metrics, and traces. We now describe the architecture and\\ncapabilities of the MCP server in more detail, using the implementation provided by Comet’s Opik platform\\nand its open-source Opik MCP server (Comet, 2025a;c;d) as an illustrative example. The goal is to show how\\nthe MCP server serves as the unifying backbone that links development (IDE), testing (CI), and runtime\\nmonitoring in an AI-first application.\\nUnified Interface and Transport: The MCP server exposes a consistent API for clients to perform operations\\nsuch as logging a trace, querying stored traces, saving or retrieving prompt templates, listing projects or\\nexperiments, and fetching aggregate metrics through cookbook-style docs and examples that the IDE can\\nuse as examples to enable logging, metrics, and telemetry. Importantly, it is designed to be accessible from\\ndifferent environments. For instance, Opik’s MCP server supports multiple transport mechanisms so it can\\nintegrate with various IDEs (Cursor, VS Code plugins, etc.) and also with non-UI clients (Comet, 2025a;b).\\nIn an IDE like Cursor, a developer can add the MCP server as a connection (specifying an API key and\\nendpoint). Thereafter, the IDE’s AI assistant can use this channel to communicate with the server for any\\ntelemetry-related query. The choice of standard protocols means the integration is relatively lightweight and\\ndoesn’t require deep changes in the IDE, a key engineering decision to allow quick adoption.\\nPrompt Management: One core function of MCP is managing prompt versions and libraries. The server can\\nstore prompts (e.g., system prompts or few-shot example sets) keyed by names or IDs. Through the MCP API,\\na user or agent can list available prompts, fetch the latest version of a prompt, or save a new version (Comet,\\n2025b). This is extremely useful for keeping track of prompt iterations. For example, when a developer\\naccepts an optimized prompt recommendation (as in Figure 5), the IDE could save that new prompt to the\\nMCP server with a version tag or comment. Now the CI pipeline and other team members have access to it.\\nConversely, if the CI’s automated optimizer finds a better prompt, it could update the repository and also call\\nthe MCP API to record this prompt. Storing prompts centrally also enables prompt reuse and templating\\nacross projects – teams can build a library of tested prompts for common tasks (summarization, Q&A, etc.),\\nqueryable via MCP. This concept, enabling the creation of libraries of tested and reusable prompts, is also\\nrelated to ideas like \"Prompt Baking\" (Bhargava et al., 2024).\\nTrace Logging and Analysis: The MCP server logs traces for each interaction or sequence of interactions as\\ndirected by the application. A trace typically includes the prompt used (or chain of prompts in an agent), the\\nmodel’s output, any tool calls and their results (for agent scenarios), and metadata like timestamps, model\\nidentity, and evaluation scores. Opik’s implementation provides methods to log various types of traces such\\nas basic LLM calls, multi-step agent traces, conversation logs, and even distributed traces that tie together\\nmulti-component processes (Comet, 2025a). This flexibility suggests that MCP is meant to capture not only\\nsingle-turn prompt-response pairs but also complex graphs of events. For analysis, the MCP server can be\\nqueried with filters (e.g., by time range, by specific prompt or scenario, or by an evaluation metric threshold).\\nAs described earlier, the server can answer questions like \"How many traces have been logged to the ’demo’\\nproject?\" or \"What is the latest trace’s output?\" directly (Comet, 2025b). These queries abstract away the\\nunderlying database queries. The user (or LLM agent) can simply ask in natural language via the IDE, and\\nthe MCP handles retrieving the info to feed into the LLM’s answer. Additionally, the server can perform\\nsimple analyses like counting occurrences of events or computing average token usage, which can then be\\npresented as part of telemetry summaries.\\nIt’s worth noting that because MCP can store evaluation results alongside traces, the queries can combine\\nthem. For example, \"search for the most recent traces with high hallucination scores in project X\" (Comet,\\n10'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Vincent Koc; Jacques Verre; Douglas Blank; Abigail Morgan', 'doi': 'https://doi.org/10.48550/arXiv.2506.11019', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Mind the Metrics: Patterns for Telemetry-Aware In-IDE AI Application Development using the Model Context Protocol (MCP)', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.11019v1', 'source': 'd:\\\\GITHUB Projects\\\\Document_portal\\\\notebook\\\\data\\\\mind the metrics.pdf', 'total_pages': 16, 'page': 10, 'page_label': '11'}, page_content='AI\\nApplication\\nMetrics & Telemetry\\nStore (Opik)\\nMCP Server\\n(Opik MCP)\\nMCP Client\\n(AI-Assisted IDE)\\n1. trace\\n2. getMetrics()\\n3. query\\n4. metrics / traces\\n5. metrics / traces\\n6. LLM Decision:\\nRefine Prompt\\nTool-call\\nexecution\\nFigure 4: Telemetry round-trip message sequence. (1) Application streams a trace to Metrics & Telemetry\\nStore (Opik). (2) The IDE/LLM requests metrics via the MCP client, which queries Opik Store (3). (4)-(5)\\nMetrics/traces flow back to the IDE/LLM, where (6) a local LLM tool-call can refine prompts based on the\\nreceived telemetry.\\n2025b) is a query that relies on an evaluation metric (hallucination score) being logged for each trace. The\\nMCP server thus functions as a search engine over both raw interaction data and derived metrics. This\\ncapability exemplifies why unifying these concerns (Metrics, Prompt, and Control) is powerful: one can\\ncorrelate prompt versions with metrics easily, or find problematic outputs and directly retrieve the prompts\\nthat led to them.\\nMetrics and Project Dashboard: In addition to individual traces, the MCP server maintains aggregate project\\nmetrics. A project in this context might correspond to an application or an experiment (e.g., \"Banking\\nCustomer Support Chatbot v1\"). Metrics could include things like average response time, success rate of\\nqueries, and token consumption over time. These can be fetched via the API for building dashboards or\\ntriggering alerts. Opik’s MCP server documentation mentions \"fetching project metrics\" as part of the\\ninterface (Comet, 2025b). One could imagine a CI pipeline querying these metrics after a load test to decide\\nif the latest deployment meets performance targets. Likewise, a monitor agent might ask for a trend (e.g.,\\n\"has the satisfaction score improved this week?\"), which the MCP might answer by providing stats if such a\\nmetric is tracked. By centralizing metrics, MCP avoids the duplication of instrumentation code; every client,\\nbe it the IDE, CI, or an external monitoring service, taps into the same store.\\nControl and Tooling Integration: The \"Control\" aspect of the MCP telemetry pattern is perhaps the most\\nforward-looking. In the current implementation, control is implicit in the sense that one can save prompts or\\ninitiate evaluations via the MCP API, actions that cause changes. However, one can envision more explicit\\ncontrol messages. For instance, a command could be sent to switch the active prompt for an agent to a\\ndifferent version for A/B testing prompts. Or a control signal could pause an agent if a certain metric\\nthreshold is exceeded, like halting an agent that has gone into an infinite loop, based on trace analysis. While\\nthese are not detailed in the documentation we have, the architecture allows insertion of such commands\\nsince the MCP server sits between the IDE (or other clients) and the model execution environment.\\nA practical example of control could be the Agent Development Kit (ADK) integration mentioned earlier.\\nThrough MCP integrating with a given agentic framework, developers are allowed to start/stop agents and\\nget full trace visibility from the IDE (Comet, 2025a;b). This implies a control channel where the IDE can send\\nan instruction like \"launch agent with config X and feed outputs to server.\" Similarly, LLMs with function\\ncalling, such as OpenAI’s, can be monitored and perhaps controlled (e.g., disabling a function/tool if it leads\\nto errors) via such a protocol.\\n11'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Vincent Koc; Jacques Verre; Douglas Blank; Abigail Morgan', 'doi': 'https://doi.org/10.48550/arXiv.2506.11019', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Mind the Metrics: Patterns for Telemetry-Aware In-IDE AI Application Development using the Model Context Protocol (MCP)', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.11019v1', 'source': 'd:\\\\GITHUB Projects\\\\Document_portal\\\\notebook\\\\data\\\\mind the metrics.pdf', 'total_pages': 16, 'page': 11, 'page_label': '12'}, page_content='From a systems perspective, the MCP server can be seen as middleware in the LLM AI toolchain. It doesn’t\\nreplace the LLM or the application’s code; instead, it instruments and mediates. By doing so, it decouples\\noptimization and analysis from the core application logic. This decoupling is essential for plugging in external\\noptimizers or validators. For example, one could run a separate process that periodically queries MCP\\nfor recent performance metrics and, if certain conditions are met, calls an optimizer library (like a DSPy\\nMIPRO optimizer) to generate a better prompt, which it then submits back to MCP and possibly to a\\nrepository. The application using the prompt might then be notified or can pull the updated prompt at its\\nnext startup. Because MCP provides a version-controlled prompt store, rolling back is also straightforward if\\nan optimization misfires, as one can revert to an earlier prompt version.\\nSecurity and permissions are also an architectural consideration (though beyond our scope to detail): since\\nMCP centralizes access to potentially sensitive data (model inputs/outputs, user queries, etc.), it needs access\\ncontrol. The MCP design discussed by Hou et al. (2025) suggests phases like creation and update include\\nconsiderations of privacy and security. In practice, API keys, role-based permissions (who can query vs. who\\ncan modify prompts) would be part of a robust MCP server deployment. For an open-source tool like Opik’s\\nserver, developers can self-host it and thus control their data, or use a cloud service with guarantees.\\nIn summary, the MCP server architecture provides:\\n• A single source of truth for prompts and telemetry data.\\n• Standardized queries for analysis and debugging.\\n• Interoperability between different stages of development (IDE, CI, production monitoring).\\n• Extensibility for future integration of optimizers and automated agents.\\nBy emphasizing architecture over any single algorithm, we allow the ecosystem to evolve. Today’s state-of-\\nthe-art optimizers (e.g., MIPROv2 in DSPy (Khattab et al., 2023) or PromptWizard (Agarwal et al., 2024))\\ncan be tomorrow’s legacy, as new ones will emerge. But with an MCP-based design, the AI development\\nworkflow does not need to be reinvented for each new optimizer. The optimizers can simply hook into the\\nsame telemetry streams and update the same prompt repositories. This unification is analogous to how the\\nrise of logging frameworks and A/B testing platforms in web development allowed many different analytics or\\noptimization modules to plug in without modifying the core app. We foresee the MCP paradigm playing a\\nsimilar enabling role for AI-first software.\\n5 Discussion and Future Directions\\nThe telemetry-aware IDE paradigm outlined here is in an early stage of adoption, but it signals a broader\\nshift in how we conceive of AI development workflows. In this section, we discuss some implications of this\\nparadigm, potential challenges, and future research directions that emerge from our proposal.\\nChanging Developer Roles and Skills: If IDEs become observability-rich AI development platforms, the skill\\nset required for prompt/agent engineering may evolve. Developers will not only write prompts or glue code\\nbut also define metrics, interpret telemetry dashboards, and perhaps write meta-prompts to query their own\\nsystems. The workflow begins to resemble a dialogue with the AI system: you build the system, then ask the\\nsystem about itself to improve it. This could lower the barrier for debugging AI behavior, since you can ask in\\nnatural language for summaries of failures, but it also means developers must think in terms of experiments\\nand data analysis. Future studies could examine how developers adapt to these tools. For example, does\\nseeing quantitative metrics in the IDE lead to more objective prompt tuning decisions? Does having a\\nconversational interface to logs (as shown in Figure 5) make debugging more accessible to non-programmers?\\nThese human factors questions will be important for tool designers.\\nBenchmarking the Paradigm: We intentionally avoided any performance claims in this paper, focusing on\\nconceptual and architectural benefits. However, a logical next step is to empirically evaluate telemetry-aware\\nworkflows. One could set up user studies where some developers use a telemetry-integrated IDE and others\\nuse a standard IDE to accomplish the same AI task, measuring differences in success rates or time. On the\\n12'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Vincent Koc; Jacques Verre; Douglas Blank; Abigail Morgan', 'doi': 'https://doi.org/10.48550/arXiv.2506.11019', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Mind the Metrics: Patterns for Telemetry-Aware In-IDE AI Application Development using the Model Context Protocol (MCP)', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.11019v1', 'source': 'd:\\\\GITHUB Projects\\\\Document_portal\\\\notebook\\\\data\\\\mind the metrics.pdf', 'total_pages': 16, 'page': 12, 'page_label': '13'}, page_content='system side, one could benchmark how quickly an automated optimizer converges when it has access to live\\ntelemetry versus when it’s run offline on a static dataset. The expectation is that tighter feedback loops\\nyield faster iteration and more robust solutions, but this needs validation. Particularly interesting would be\\nmeasuring the impact of autonomous monitoring agents – can they actually catch and fix issues faster than\\nhuman operators? To enable such research, common evaluation tasks and datasets for \"AI self-debugging\"\\ncould be developed (e.g., a suite of known prompt pitfalls that an autonomous agent should detect and\\ncorrect).\\nIntegration with LLM Evaluation Research: Our paradigm intertwines with the complex field of LLM\\nevaluation – using LLMs as judges or designing new metrics for quality. The telemetry platform provides\\nthe data for evaluation, but what metrics to compute and how to interpret them is a whole challenge on\\nits own. An open question is: Which telemetry signals are most indicative of real performance issues? It\\ncould be the frequency of user corrections, or latency spikes, or certain embeddings drifting – the MCP can\\nlog it all, but someone has to decide what triggers an optimization. Research into metric design (especially\\ncomposite metrics for things like \"helpfulness\" or \"harmlessness\" of an agent) will directly feed into how\\neffective telemetry-aware development can be. If the metrics are poor, developers could be misled by the\\ntelemetry (optimizing for the wrong thing). This calls for careful design of evaluation criteria and possibly\\ninteractive tuning of those criteria.\\nOrchestration and Interoperability: As multiple tools (IDE, CI, monitoring agents, optimizers) interconnect\\nvia MCP, orchestration becomes important. There may be scenarios of conflicting suggestions, for example,\\nan automated optimizer proposes a prompt change that the monitoring agent disagrees with based on\\nrecent user feedback. How do we reconcile different \"advisors\" to the development process? One approach\\ncould be a central policy or meta-agent that takes input from various sources (human developers, CI tests,\\nmonitor agents) and makes decisions. This starts to look like an AI development orchestration layer, which\\ncould be another LLM or a rule-based system. We are essentially building an ecosystem where parts of the\\nsoftware development lifecycle are AI-driven (e.g., tests, analysis, optimization), and these need coordination.\\nEstablishing standards, possibly extensions of MCP, for how these components communicate their suggestions\\nand outcomes will be useful. The current MCP focuses on context and telemetry; future iterations might\\ninclude a schema for \"proposed changes\" or \"hypotheses\" that agents can submit.\\nLimitations and Risks: A discussion of this new paradigm must acknowledge potential pitfalls. One risk is\\nover-reliance on automated feedback, as developers might trust the AI’s self-evaluation too much. If an LLM\\nsays, \"I have improved the prompt and errors are down by 50%\", a developer might be tempted to accept that\\nat face value. But perhaps the evaluation metric was incomplete. Ensuring a human-in-the-loop for validation,\\nespecially for changes that go to production, is advisable at the current state of technology. Another risk is\\ndata privacy and security: telemetry data can include sensitive user inputs or model outputs. Centralizing\\nit in an MCP server means one must secure that server and comply with data handling norms, such as\\nanonymization or encryption. The more we integrate these systems from IDE to production, the higher\\nthe stakes if there’s a leak or misuse. Techniques like redaction of sensitive info in traces or limiting access\\nscope will be important to implement alongside the functional features. Furthermore, prompt optimization\\nalgorithms can be time-consuming, and the associated latency might detract from the developer experience\\nat present.\\nFuture Optimizer Integration: One of our core motivations was to pave the way for integrating any advanced\\noptimizerintotheworkflowwithoutfriction. WetouchedonhowDSPy(Khattabetal.,2023)orPromptWizard\\n(Agarwal et al., 2024) could plug in. Looking ahead, optimizers might evolve to use reinforcement learning\\nor advanced search that runs continuously. For a more detailed example of such a background service, see\\nAppendix B.\\nCommunity and Standards: Finally, to truly realize telemetry-aware AI development at large, community\\nstandards and best practices should form. The Model Context Protocol itself is a candidate for standardization\\n(Hou et al., 2025; Anthropic, 2024). If multiple vendors and open-source projects adopt a common protocol,\\nIDEs and tools can interoperate. For instance, one could use VS Code with an MCP plugin that works\\nwith any compliant telemetry and metrics server. This would echo how LSP (Language Server Protocol)\\nstandardized language tooling integration in IDEs. We might see an LLM Telemetry Protocol as an extension\\n13'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Vincent Koc; Jacques Verre; Douglas Blank; Abigail Morgan', 'doi': 'https://doi.org/10.48550/arXiv.2506.11019', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Mind the Metrics: Patterns for Telemetry-Aware In-IDE AI Application Development using the Model Context Protocol (MCP)', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.11019v1', 'source': 'd:\\\\GITHUB Projects\\\\Document_portal\\\\notebook\\\\data\\\\mind the metrics.pdf', 'total_pages': 16, 'page': 13, 'page_label': '14'}, page_content='of MCP focusing on observability data interchange. Additionally, sharing of telemetry datasets (anonymized)\\ncould spur innovation: imagine a public repository of agent trace logs and outcomes that researchers use\\nto test new evaluation metrics or optimization methods. We encourage the community to explore such\\ncollaborative efforts, as the challenges of aligning AI behavior are too vast for siloed solutions.\\n6 Conclusion\\nWe have presented a vision for observability-first AI IDEs centered around a unified Metrics, Control, Prompt\\n(MCP) pattern, facilitated by the Model Context Protocol. By weaving real-time telemetry and feedback loops\\ninto the fabric of development tools, this paradigm enables a new level of insight and iterative improvement in\\nAI application development. We described design patterns ranging from immediate in-IDE metrics feedback\\nto fully autonomous self-optimization agents, all supported by an architectural backbone that treats prompts\\nand their telemetry as first-class artifacts.\\nThis approach reframes prompt engineering and agent design from a static art into a dynamic, data-driven\\nprocess. It draws on ideas from prompt optimization research (such as treating prompts as programs and\\nusing AI critics) and from traditional software observability (like instrumentation and monitoring), unifying\\nthem in a practical development workflow. The MCP server exemplifies how such unification can be achieved:\\nby providing a common interface for logging, querying, and updating AI context, it bridges the gap between\\ndevelopment, testing, and production monitoring.\\nWe emphasize that this is a theoretical and conceptual contribution. The potential benefits, such as faster\\ndebugging of AI issues, continuous improvement of model behavior, and safer deployments through constant\\nevaluation, are compelling, but realizing them will require further experimentation and user studies. As a\\nnext step, the insights from this paper can inform the implementation of telemetry-aware features in popular\\nIDEs and AI development frameworks. We envision that in the near future, asking your IDE \"how is my\\nAI model performing?\" will be as natural as checking your code for compile errors. When that happens,\\nthe journey from debug logs to dynamic prompts will have come full circle, fulfilling the promise of truly\\nintelligent development environments.\\nUltimately, telemetry-aware AI-first IDEs represent a convergence of software engineering and AI engineering\\npractices. Embracing this paradigm could lead to more robust, transparent, and adaptable AI systems –\\nsystems that not only learn from data but also learn from their own operation, in partnership with human\\ndevelopers. We hope this work provides a foundation for that evolution and inspires further innovation in\\nbuilding the next generation of AI development tools.\\nReferences\\nX. Hou, Y. Zhao, S. Wang, and H. Wang. Model Context Protocol (MCP): Landscape, Security Threats, and\\nFuture Research Directions.arXiv:2503.23278, 2025. https://arxiv.org/abs/2503.23278.\\nA. Sergeyuk, S. Titov, and M. Izadi. In-IDE Human-AI Experience in the Era of Large Language Models; A\\nLiterature Review. arXiv:2401.10739, 2024. https://arxiv.org/abs/2401.10739.\\nO. Anuyah, K. Badillo-Urquiola, and R. Metoyer. Characterizing the Technology Needs of Vulnerable\\nPopulations for Participation in Research and Design by Adopting Maslow’s Hierarchy of Needs. In\\nProceedings of the 2023 CHI Conference on Human Factors in Computing Systems (CHI ’23). Association\\nfor Computing Machinery, New York, NY, USA, Article 85, 1–20, 2023.https://doi.org/10.1145/3544\\n548.3581221.\\nA. Y. Wang, D. Wang, J. Drozdal, M. Muller, S. Park, J. D. Weisz, X. Liu, L. Wu, and C. Dugan.\\nDocumentation Matters: Human-Centered AI System to Assist Data Science Code Documentation in\\nComputational Notebooks. ACM Trans. Comput.-Hum. Interact., 29(2), Article 17, 1–33, 2022.https:\\n//doi.org/10.1145/3489465.\\n14'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Vincent Koc; Jacques Verre; Douglas Blank; Abigail Morgan', 'doi': 'https://doi.org/10.48550/arXiv.2506.11019', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Mind the Metrics: Patterns for Telemetry-Aware In-IDE AI Application Development using the Model Context Protocol (MCP)', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.11019v1', 'source': 'd:\\\\GITHUB Projects\\\\Document_portal\\\\notebook\\\\data\\\\mind the metrics.pdf', 'total_pages': 16, 'page': 14, 'page_label': '15'}, page_content='O. Khattab, A. Singhvi, P. Maheshwari, Z. Zhang, K. Santhanam, S. Vardhamanan, S. Haq, A. Sharma,\\nT. T. Joshi, H. Moazam, H. Miller, M. Zaharia, and C. Potts. DSPy: Compiling Declarative Language\\nModel Calls into Self-Improving Pipelines.arXiv:2310.03714, 2023.https://arxiv.org/abs/2310.03714.\\nT. Schnabel and J. Neville. Prompts As Programs: A Structure-Aware Approach to Efficient Compile-Time\\nPrompt Optimization. Microsoft Research Technical Report, MSR-TR-2024-XX, April 2024.https:\\n//www.microsoft.com/en-us/research/publication/prompts-as-programs-a-structure-aware-a\\npproach-to-efficient-compile-time-prompt-optimization/ . (Accessed: May 2025).\\nE. Agarwal, J. Singh, V. Dani, R. Magazine, T. Ganu, and A. Nambi. PromptWizard: Task-Aware Prompt\\nOptimization Framework.arXiv:2405.18369, 2024. https://arxiv.org/abs/2405.18369.\\nComet. Product Releases – April 2025. Comet Blog, 2025.https://www.comet.com/site/blog/comet-p\\nroduct-releases-april2025/. (Accessed: May 2025).\\nComet. Opik Documentation – MCP Server. Comet Documentation, 2025.https://www.comet.com/docs\\n/opik/prompt_engineering/mcp_server. (Accessed: May 2025).\\nManagement Solutions. The Rise of LLMs and the Emergence of LLMOps. White Paper, Management\\nSolutions, 2023. https://www.managementsolutions.com/sites/default/files/minisite/static/\\n72b0015f-39c9-4a52-ba63-872c115bfbd0/llm/pdf/rise-of-llm.pdf . (Accessed: May 2025).\\nC. Shi, P. Liang, Y. Wu, T. Zhan, and Z. Jin. Maximizing User Experience with LLMOps-Driven Personalized\\nRecommendation Systems. arXiv:2404.00903, 2024. https://arxiv.org/abs/2404.00903.\\nS. Pahune and Z. Akhtar. Transitioning from MLOps to LLMOps: Navigating the Unique Challenges of Large\\nLanguage Models. Information, 16(2), Article 87, 2025.https://www.mdpi.com/2078-2489/16/2/87 .\\n(Accessed: May 2025).\\nM. Sinha, S. Menon, and R. Sagar. LLMOps: Definitions, Framework and Best Practices. In 2024\\nInternational Conference on Electrical, Computer and Energy Technologies (ICECET), 1–6, 2024.https:\\n//doi.org/10.1109/ICECET61485.2024.10698359. (Accessed: May 2025).\\nZencoder.ai. Agentic Pipeline Product Information. Website, 2024.https://zencoder.ai/product/agen\\ntic-pipeline. (Accessed: May 2025).\\nComet ML, Inc. Opik MCP Server. GitHub Repository, 2025. (Primary development by V. Koc, Y. Boiko).\\nhttps://github.com/comet-ml/opik-mcp. (Accessed: May 2025).\\nComet ML, Inc. Opik MCP Server. Zenodo, 2025. (Archival of work by V. Koc, Y. Boiko). https:\\n//doi.org/10.5281/zenodo.15411156.\\nA. Bhargava, C. Witkowski, A. Detkov, and M. Thomson. Prompt Baking. arXiv:2409.13697, 2024.\\nhttps://arxiv.org/abs/2409.13697.\\nAnthropic. Introducing the Model Context Protocol. News, Anthropic, Nov 25, 2024.https://www.anthro\\npic.com/news/model-context-protocol. (Accessed: May 2025).\\nV. Koc. Generative AI Design Patterns: A Comprehensive Guide. Towards Data Science (Medium),\\nFeb. 14, 2024. https://towardsdatascience.com/generative-ai-design-patterns-a-compreh\\nensive-guide-41425a40d7d0 , also available athttps://medium.com/data-science/generativ\\ne-ai-design-patterns-a-comprehensive-guide-41425a40d7d0 . (Accessed: May 2025). https:\\n//doi.org/10.5281/zenodo.14769623.\\n15'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Vincent Koc; Jacques Verre; Douglas Blank; Abigail Morgan', 'doi': 'https://doi.org/10.48550/arXiv.2506.11019', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Mind the Metrics: Patterns for Telemetry-Aware In-IDE AI Application Development using the Model Context Protocol (MCP)', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.11019v1', 'source': 'd:\\\\GITHUB Projects\\\\Document_portal\\\\notebook\\\\data\\\\mind the metrics.pdf', 'total_pages': 16, 'page': 15, 'page_label': '16'}, page_content='A Example IDE Telemetry Interaction\\nPlaceholder for preprint: Final figure to illustrate telemetry insights and an optimized prompt\\nsuggestion generated within the AI-assistedIDE via an integrated MCP server. The figure will\\nshow how a developer, after asking for analysis based on recent trace logs for an application (e.g.,\\n\"Business Banking Customer Service Agent\"), receives a summary of Telemetry Insights (such as\\nproject focus, token usage statistics, response time distribution, conversation flow observations)\\nand an Optimized LLM Prompt Recommendation. This demonstrates how real-time metrics and\\ntraces can be queried in natural language and leveraged to improve prompts on the fly.\\nFigure 5: Example telemetry insights and optimized prompt suggestion in an IDE. [Placeholder for preprint:\\nFinal figure to be included.]\\nB Elaboration on Future Optimizer Integration Concepts\\nAn interesting idea is a background optimizer service that constantly runs experiments in parallel (using\\nspare compute, perhaps) to try and improve prompts or model parameters. MCP could feed it a stream of\\nrandom real queries (privacy permitting) and the optimizer could test slight prompt variations to see if it\\nyields better answers, all logged under a separate project or sandbox. If it discovers something better, it\\ncould alert developers. This resembles AutoML (automated model tuning) but applied to prompts/policies.\\nWith the telemetry infra, such experimental branches can be safely conducted without affecting users until\\nconfirmed. We anticipate research on online prompt optimization algorithms that use live traffic in a safe way\\n(multi-armed bandit approaches, for example, to slowly route a small percentage of requests to a candidate\\nprompt to gather metrics). The MCP’s unified logging would capture both control and experimental groups\\nfor analysis.\\n16')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a88dc1c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "64ddf341",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter=RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=150,\n",
    "    length_function=len\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c25516d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs=text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b0461ebf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "197"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "60b09c81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Vincent Koc; Jacques Verre; Douglas Blank; Abigail Morgan', 'doi': 'https://doi.org/10.48550/arXiv.2506.11019', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Mind the Metrics: Patterns for Telemetry-Aware In-IDE AI Application Development using the Model Context Protocol (MCP)', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.11019v1', 'source': 'd:\\\\GITHUB Projects\\\\Document_portal\\\\notebook\\\\data\\\\mind the metrics.pdf', 'total_pages': 16, 'page': 0, 'page_label': '1'}, page_content='neering with live metrics, traces, and evaluations to enable iterative optimization and robust\\nmonitoring. We present a progression of design patterns: from local large language model\\n(LLM) coding with immediate metrics feedback, to continuous integration (CI) pipelines\\nthat automatically refine prompts, to autonomous agents that monitor and adapt prompts\\nbased on telemetry. Instead of focusing on any single optimizer, we emphasize a general')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "51a857a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'producer': 'pikepdf 8.15.1',\n",
       " 'creator': 'arXiv GenPDF (tex2pdf:)',\n",
       " 'creationdate': '',\n",
       " 'author': 'Vincent Koc; Jacques Verre; Douglas Blank; Abigail Morgan',\n",
       " 'doi': 'https://doi.org/10.48550/arXiv.2506.11019',\n",
       " 'license': 'http://creativecommons.org/licenses/by/4.0/',\n",
       " 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5',\n",
       " 'title': 'Mind the Metrics: Patterns for Telemetry-Aware In-IDE AI Application Development using the Model Context Protocol (MCP)',\n",
       " 'trapped': '/False',\n",
       " 'arxivid': 'https://arxiv.org/abs/2506.11019v1',\n",
       " 'source': 'd:\\\\GITHUB Projects\\\\Document_portal\\\\notebook\\\\data\\\\mind the metrics.pdf',\n",
       " 'total_pages': 16,\n",
       " 'page': 0,\n",
       " 'page_label': '1'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[2].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c6cce0c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'neering with live metrics, traces, and evaluations to enable iterative optimization and robust\\nmonitoring. We present a progression of design patterns: from local large language model\\n(LLM) coding with immediate metrics feedback, to continuous integration (CI) pipelines\\nthat automatically refine prompts, to autonomous agents that monitor and adapt prompts\\nbased on telemetry. Instead of focusing on any single optimizer, we emphasize a general'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "docs[2].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3886e2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9424180e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3899746f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71dddfc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59ce4d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a6e5aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09cf0c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084a9502",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b40327",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5fa969",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41af8a5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301a2d67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc515f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
